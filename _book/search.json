[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Factor Investing mit Python: Eine Einführung",
    "section": "",
    "text": "1 Willkommen zu „Factor Investing mit Python: Eine Einführung“",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zu „Factor Investing mit Python: Eine Einführung“</span>"
    ]
  },
  {
    "objectID": "index.html#motivation-und-zielsetzung",
    "href": "index.html#motivation-und-zielsetzung",
    "title": "Factor Investing mit Python: Eine Einführung",
    "section": "1.1 Motivation und Zielsetzung",
    "text": "1.1 Motivation und Zielsetzung\nIn den letzten Jahren hat das sogenannte Factor Investing sowohl in der akademischen Forschung als auch in der Praxis des Portfoliomanagements eine zentrale Bedeutung erlangt. Institutionelle Investoren, Hedgefonds und Vermögensverwalter setzen zunehmend auf faktorbasiertes Investieren, um systematisch Renditequellen zu identifizieren und Portfolios effizient zu strukturieren.\nZiel dieses Buches ist es, die theoretischen Grundlagen des Factor Investings zu vermitteln und deren praktische Umsetzung in Python Schritt für Schritt zu demonstrieren. Dabei geht es nicht nur um das Verständnis einzelner Faktoren wie Momentum oder IVOL, sondern insbesondere um die Frage, wie sich diese Konzepte konkret mit realen Daten operationalisieren und in einem Portfolio umsetzen lassen.\nDas Buch versteht sich als praxisorientierte Einführung in die empirische Umsetzung von Faktorstrategien. Es richtet sich an fortgeschrittene Masterstudierende, Young Professionals und berufstätige Quants, die ihr Wissen im Bereich quantitativer Investmentstrategien vertiefen und mit Python-basierter Implementierungspraxis verbinden möchten.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zu „Factor Investing mit Python: Eine Einführung“</span>"
    ]
  },
  {
    "objectID": "index.html#aufbau-des-buches",
    "href": "index.html#aufbau-des-buches",
    "title": "Factor Investing mit Python: Eine Einführung",
    "section": "1.2 Aufbau des Buches",
    "text": "1.2 Aufbau des Buches\nDas Buch ist modular aufgebaut und gliedert sich in sechs Kapitel.\n\nWillkommen zu „Factor Investing mit Python: Eine Einführung“\nDieses Kapitel gibt einen Überblick über Motivation, Zielgruppe und Aufbau des Buches. Zudem wird erläutert, wie der Python-Code und die Datensätze verwendet werden können.\nGrundlagen des Factor Investings\nHier werden die theoretischen Grundlagen vorgestellt: Was ist ein Faktor? Wie entstehen Faktorprämien? Welche empirischen Befunde gibt es zur Existenz und Stabilität von Faktor-Renditen?\nPreisbasierte Faktoren: Momentum und Reversal\nDieses Kapitel behandelt zwei der am häufigsten genutzten renditebasierten Faktoren – Momentum (Fortsetzung vergangener Trends) und Short-Term Reversal (Umkehr kurzfristiger Trends) – und zeigt, wie sie auf Aktienrenditen angewandt werden.\nRisikobasierte Faktoren: Total Risk\nHier geht es um Faktoren, die auf unterschiedlichen Risikomaßen basieren. Anhand der Total Volatility wird untersucht, ob risikoarme Aktien systematisch höhere risikoadjustierte Renditen erzielen.\nRisikobasierte Faktoren: Idiosyncratic Volatility (IVOL)\nDieses Kapitel erklärt den IVOL-Faktor, der auf der unternehmensspezifischen (nicht-systematischen) Volatilität basiert. Es wird gezeigt, wie IVOL aus Regressionsmodellen abgeleitet und als Faktor konstruiert wird.\nRisikobasierte Faktoren: Stock Beta\nAbschließend wird der Betting-against-Beta (BAB)-Ansatz erläutert, bei dem die Überrenditen von Low-Beta-Aktien gegenüber High-Beta-Aktien untersucht werden. Auch hier werden die theoretischen Überlegungen empirisch mit Python überprüft.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zu „Factor Investing mit Python: Eine Einführung“</span>"
    ]
  },
  {
    "objectID": "index.html#lernziele",
    "href": "index.html#lernziele",
    "title": "Factor Investing mit Python: Eine Einführung",
    "section": "1.3 Lernziele",
    "text": "1.3 Lernziele\nNach der Lektüre des Buches sollen die Leserinnen und Leser in der Lage sein:\n\nFaktorstrategien theoretisch zu verstehen und deren Relevanz im modernen Asset Management zu beurteilen.\n\nAktienbasierte Faktormodelle in Python zu implementieren, zu testen und zu analysieren.\n\nEigene Faktorportfolios zu konstruieren und die Performance anhand standardisierter Kennzahlen zu evaluieren.\n\nReale Marktdaten in Python zu verarbeiten und robuste Backtesting-Prozesse aufzubauen.\n\nDie Stärken und Grenzen faktorbasierten Investierens kritisch einzuordnen.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zu „Factor Investing mit Python: Eine Einführung“</span>"
    ]
  },
  {
    "objectID": "index.html#praktischer-nutzen-und-lernansatz",
    "href": "index.html#praktischer-nutzen-und-lernansatz",
    "title": "Factor Investing mit Python: Eine Einführung",
    "section": "1.4 Praktischer Nutzen und Lernansatz",
    "text": "1.4 Praktischer Nutzen und Lernansatz\nDas Buch legt großen Wert auf die praktische Anwendung. Theoretische Konzepte werden stets mit empirischen Fallstudien verknüpft. Jeder Abschnitt enthält ausführlich kommentierten Python-Code, der es den Lesern ermöglicht, die Beispiele nachzuvollziehen, zu verändern und zu erweitern.\nDie Fallstudien sind so gestaltet, dass sie auf realen Marktdaten basieren – typischerweise Kurs- und Renditedaten börsennotierter Aktien. Damit wird der Brückenschlag zwischen Theorie und Praxis besonders deutlich:\nWie kann man Faktorprämien empirisch messen? Wie sieht die tatsächliche Performance einer Momentum- oder IVOL-Strategie aus? Und wie robust sind die Ergebnisse über Zeiträume und Märkte hinweg?\nDie Python-Implementierungen verwenden dabei ausschließlich frei verfügbare Bibliotheken, insbesondere:\n\npandas zur Datenverarbeitung,\n\nnumpy für numerische Berechnungen,\n\nmatplotlib und seaborn für Visualisierungen,\n\noptional statsmodels für Regressionsanalysen.\n\nAlle Beispiele sind so konzipiert, dass sie ohne externe Datenabonnements (z. B. Bloomberg oder Refinitiv) nachvollzogen werden können. Typischerweise werden frei verfügbare historische Daten (z. B. über Yahoo Finance) verwendet.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zu „Factor Investing mit Python: Eine Einführung“</span>"
    ]
  },
  {
    "objectID": "index.html#zum-praktischen-arbeiten-mit-dem-buch",
    "href": "index.html#zum-praktischen-arbeiten-mit-dem-buch",
    "title": "Factor Investing mit Python: Eine Einführung",
    "section": "1.5 Zum praktischen Arbeiten mit dem Buch",
    "text": "1.5 Zum praktischen Arbeiten mit dem Buch\nDas Buch kann auf zwei Weisen genutzt werden:\n\nAls Webbuch – zum interaktiven Lesen und Nachvollziehen der Inhalte im Browser.\n\nAls interaktive Python-Umgebung – die mitgelieferten Jupyter-Notebooks (.ipynb) können direkt in einer IDE (z. B. Visual Studio Code, JupyterLab oder Google Colab) geöffnet werden.\n\nEmpfohlen wird, die Kapitel parallel mit Python zu bearbeiten. Die Lerneffizienz steigt erheblich, wenn die Codebeispiele nicht nur gelesen, sondern tatsächlich ausgeführt und variiert werden.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zu „Factor Investing mit Python: Eine Einführung“</span>"
    ]
  },
  {
    "objectID": "index.html#lizenz-daten-und-urheberrecht",
    "href": "index.html#lizenz-daten-und-urheberrecht",
    "title": "Factor Investing mit Python: Eine Einführung",
    "section": "1.6 Lizenz, Daten und Urheberrecht",
    "text": "1.6 Lizenz, Daten und Urheberrecht\nDie Python-Notebooks und begleitenden Datensätze sind ausschließlich für Bildungs- und Forschungszwecke bestimmt.\nAlle Rechte an Texten, Abbildungen und Codebeispielen liegen bei Univ.-Prof. Dr. Thomas Mählmann.\nEin kommerzieller Gebrauch oder eine Weitergabe an Dritte ist nur mit ausdrücklicher Genehmigung des Autors zulässig.\nZitate aus dem Buch sind unter Angabe der Quelle erlaubt.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zu „Factor Investing mit Python: Eine Einführung“</span>"
    ]
  },
  {
    "objectID": "index.html#ein-abschließendes-wort",
    "href": "index.html#ein-abschließendes-wort",
    "title": "Factor Investing mit Python: Eine Einführung",
    "section": "1.7 Ein abschließendes Wort",
    "text": "1.7 Ein abschließendes Wort\nFactor Investing ist ein faszinierendes Gebiet zwischen Kapitalmarktforschung, quantitativer Modellierung und praktischer Umsetzung im Asset Management.\nEs verbindet theoretische Einsichten aus der Finanzökonomik mit datengetriebenen Methoden, wie sie im modernen Quantitative Finance zum Einsatz kommen.\nDieses Buch möchte Ihnen einen soliden Einstieg in dieses Themenfeld geben – wissenschaftlich fundiert, aber zugleich praxisnah.\nWenn Sie die Inhalte aktiv mitarbeiten, die Beispiele nachvollziehen und eigene Ideen erproben, werden Sie am Ende in der Lage sein, eigene Faktormodelle zu entwickeln und kritisch zu bewerten.\nViel Erfolg und Freude beim Arbeiten mit den Daten, dem Code und den Konzepten dieses Buches!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Willkommen zu „Factor Investing mit Python: Eine Einführung“</span>"
    ]
  },
  {
    "objectID": "kapitel1.html",
    "href": "kapitel1.html",
    "title": "2  Grundlagen des Factor Investings",
    "section": "",
    "text": "2.1 Factor Investing als Ranking oder Long-Short Strategie\nEine Long-Short Strategie besteht darin, sowohl Long- als auch Short-Positionen aufzubauen. Bei dieser Strategie werden alle Aktien eines vorher festgelegten Universums (z.B. S&P500) anhand eines Bewertungsmaßstabs (ein Faktor) in eine Rangfolge gebracht, und die oberen \\(n\\)-Aktien der Rangfolge gekauft (Long-Position) und die unteren \\(n\\)-Aktien verkauft (Short-Position). Hierbei gilt: Gesamtwert der Long-Position = Gesamtwert der Short-Position.\nDies ist ein statistisch robuster Ansatz - durch die Rangordnung vieler Aktien und das simultane Eingehen mehrerer Positionen schließen wir faktisch zahlreiche “Wetten” auf die Prognosegüte unseres Ranking-Modells (des Faktors) ab und vermeiden so eine Abhängigkeit von wenigen riskante Wetten beim diskretionären “Stock-Picking”. Da zudem die gleiche Geldmenge in Long- und Short-Positionen investiert wird, ist die Strategie marktneutral (immun gegen Marktbewegungen).\nRanking-Schema\nEin Ranking-Schema ist jedes Modell, das beliebige Aktien in eine eindeutige Rangordnung bringen kann. Beispiele hierfür können Bewertungsrelationen (sog. Multiples), technische Indikatoren, Preismodelle oder eine Kombination aller genannten Faktoren sein. Man kann z.B. einen Momentum-Indikator verwenden, um Aktien zu reihen, oder zwei Indikatoren: Ranking-Score = (0,5 x das PE-Verhältnis der Aktie) + (0,5 x das 30-Tage-Kurs-Momentum).\nDer Erfolg dieser Strategie basiert auf der Prognosegüte des verwendeten Ranking-Schemas - die Renditen einer Long-Short Aktienstrategie hängen davon ab, wie gut das Ranking Aktien mit hohen und niedrigen zukünftigen Renditen separiert. Die Entwicklung eines profitablen Rankingschemas ist offenichtlich nicht trivial wenn Aktienmärkte effizient sind.\nAuf das Ranking Schema “wetten”\nSobald ein Modell für die Aktien-Rangordnung festgelegt wurde, kann dies verwendet werden um Handelssignale zu generieren. Ein einfacher Ansatz besteht darin, einen gleichen Geldbetrag long in die Top-Positionen der Rangliste und short in die Bottom-Positionen zu investieren. Auf diese Weise wird sichergestellt, dass sich die Strategie proportional zur Qualität der Rangordnung rentiert und zudem marktneutral ist.\nNehmen wir an, wir ordnen \\(m\\) Aktien, haben \\(d\\) Dollar zu investieren, und unsere Zielvorgabe für die Gesamtzahl der zu haltenden Positionen ist \\(2n\\). Es resultiert die folgende Strategie:\nAnmerkung: Friktion aufgrund nicht beliebiger Teilbarkeit\nWeil Aktienkurse nicht immer durch \\(\\frac{1}{2n} * d\\) gleichmäßig teilbar sind, und Aktien stückweise gekauft werden müssen, ist die obige Strategie etwas ungenau. In der Praxis ist es meistens in Ordnung, etwas mehr als \\(\\frac{1}{2n}  * d\\) Dollar pro Aktie zu kaufen. Dies kann jedoch bei niedrigen Kapitalbeträgen zu Problemen führen. Bei einer Strategie mit \\(d = 100000\\) und \\(n = 500\\) sehen wir, dass \\[\\frac{1}{2n} * d = \\frac{1}{1000} * 100000 = 100\\]\nDies wird bei teuren Aktien Probleme bereiten und dazu führen, dass der Algorithmus einen zu großen Hebel besitzt. Ein möglicher Ausweg besteht darin, weniger Aktien zu handeln oder das Kapital, \\(d\\), zu erhöhen.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen des Factor Investings</span>"
    ]
  },
  {
    "objectID": "kapitel1.html#factor-investing-als-ranking-oder-long-short-strategie",
    "href": "kapitel1.html#factor-investing-als-ranking-oder-long-short-strategie",
    "title": "2  Grundlagen des Factor Investings",
    "section": "",
    "text": "Für jede Aktie mit Rang \\(1, \\dots, n\\), verkaufe \\(\\frac{1}{2n} * d\\)-Dollar dieser Aktie\nFür jede Aktie mit Rang \\(m - n, \\dots, m\\), kaufe \\(\\frac{1}{2n} * d\\) Dollar dieser Aktie.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen des Factor Investings</span>"
    ]
  },
  {
    "objectID": "kapitel1.html#ein-hypothetisches-beispiel",
    "href": "kapitel1.html#ein-hypothetisches-beispiel",
    "title": "2  Grundlagen des Factor Investings",
    "section": "2.2 Ein hypothetisches Beispiel",
    "text": "2.2 Ein hypothetisches Beispiel\nWir generieren zufällige Aktien-Namen und einen Zufallsfaktor, nach dem wir sie anordnen. Wir gehen auch davon aus, dass zukünftige Renditen von diesen Faktorwerten abhängig sind.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\n\nCode\n# generate stocks and a random factor value for them\nstock_names = ['stock ' + str(x) for x in range(10000)]\ncurrent_factor_values = np.random.normal(0, 1, 10000)\n# generate future returns for these that are dependent on our factor values\nfuture_returns = current_factor_values + np.random.normal(0, 1, 10000)\n# put both the factor values and returns into one dataframe\ndata = pd.DataFrame(index = stock_names, columns=['Factor Value','Returns'])\ndata['Factor Value'] = current_factor_values\ndata['Returns'] = future_returns\n# take a look\ndata.head(10)\n\n\n\n\n\n\n\n\n\nFactor Value\nReturns\n\n\n\n\nstock 0\n-0.684866\n-0.364867\n\n\nstock 1\n0.020327\n-2.022530\n\n\nstock 2\n-1.165042\n-1.379441\n\n\nstock 3\n0.688505\n1.527440\n\n\nstock 4\n1.002517\n-0.000369\n\n\nstock 5\n1.308407\n1.191608\n\n\nstock 6\n-0.700452\n-1.397595\n\n\nstock 7\n-1.287748\n-1.961418\n\n\nstock 8\n0.071281\n-0.820682\n\n\nstock 9\n0.496674\n0.775194\n\n\n\n\n\n\n\nJetzt, da wir über Faktorwerte und Renditen verfügen, können wir sehen, was passieren würde, wenn wir unsere Aktien nach Faktorwerten ordnen und dann die Long- und Short-Positionen eingehen würden.\n\n\nCode\n# rank the equities\nranked_data = data.sort_values('Factor Value')\n\n# compute the returns of each basket with a basket size 500, so total (10000/500) baskets\nnumber_of_baskets = int(10000/500)\nbasket_returns = np.zeros(number_of_baskets)\n\nfor i in range(number_of_baskets):\n    start = i * 500\n    end = i * 500 + 500 \n    basket_returns[i] = ranked_data[start:end]['Returns'].mean()\n\n# plot the returns of each basket\nplt.figure(figsize=(15,7))\nplt.bar(range(number_of_baskets), basket_returns)\nplt.ylabel('Returns')\nplt.xlabel('Basket')\nplt.legend(['Returns of Each Basket'])\nplt.show()\n\n\n\n\n\n\n\n\n\nUnsere Strategie besteht darin, in den obersten Korb (Basket) long zu gehen und den untersten Korb short zu halten. Die Erträge dieser Strategie sind:\n\n\nCode\nbasket_returns[number_of_baskets-1] - basket_returns[0]\n\n\n4.077706574978949\n\n\nWir setzen grundsätzlich darauf, dass unser Ranking-Modell in der Lage ist, Aktien mit guter zukünftiger Performance von Aktien mit schlechter Performance zu trennen. Das Gute daran, Geld auf der Grundlage des Rankings zu verdienen ist, dass es vom Marktgeschehen nicht beeinflusst wird.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen des Factor Investings</span>"
    ]
  },
  {
    "objectID": "kapitel1.html#das-richtige-ranking-schema-finden",
    "href": "kapitel1.html#das-richtige-ranking-schema-finden",
    "title": "2  Grundlagen des Factor Investings",
    "section": "2.3 Das richtige Ranking-Schema finden",
    "text": "2.3 Das richtige Ranking-Schema finden\nUm eine Faktorstrategie auszuführen, müssen wir effektiv nur das Ranking-Schema festlegen. Alles was danach kommt ist rein mechanisch. Sobald wir eine Faktor- bzw. Long-Short Strategie implementiert haben, können wir verschiedene Ranking-Schemata auswählen und alles andere gleich lassen. Das ist eine sehr bequeme Methode, um Ideen, die wir haben, schnell zu wiederholen, ohne sich jedes Mal Gedanken über die Anpassung des Codes machen zu müssen.\nDie Ranking-Schemata können ebenfalls von zahlreichen Modelltypen stammen. Es muss kein Fundamentalwert- oder risikobasiertes Faktormodell sein, es können auch Methoden der Künstlichen Intelligenz (Maschinelles Lernen) verwendet werden, die die Renditen einen Monat im Voraus vorhersagen und auf dieser Grundlage eine Rangliste erstellen.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen des Factor Investings</span>"
    ]
  },
  {
    "objectID": "kapitel1.html#auswahl-und-bewertung-eines-ranking-schemas",
    "href": "kapitel1.html#auswahl-und-bewertung-eines-ranking-schemas",
    "title": "2  Grundlagen des Factor Investings",
    "section": "2.4 Auswahl und Bewertung eines Ranking-Schemas",
    "text": "2.4 Auswahl und Bewertung eines Ranking-Schemas\nDas Ranking-Schema ist die Stelle, an der eine Faktorstrategie ihren potentiellen Vorteil generiert, zugleich ist es die wichtigste Komponente. Ein erster Startpunkt für die Suche nach einem profitablen Maß ist es, bestehende bekannte Techniken auszuwählen und zu prüfen, ob sie leicht modifiziert werden können, mit dem Ziel, höhere Renditen zu erzielen. Wir werden hier einige Ansatzpunkte diskutieren.\n\nKlonen und Optimieren: Wählen Sie ein Rankingmaß aus das häufig diskutiert wird und versuchen Sie, es leicht zu modifizieren, um einen potentiellen Renditevorsprung zu gewinnen. Oftmals besitzen altbekannte Faktoren, die öffentlich zugänglich sind, keinerlei Prognosekraft mehr, da sie vom Markt schon vollständig eingepreist wurden. Manchmal zeigen sie jedoch in die richtige Richtung.\nRisikobasierte Faktoren (Preis- bzw. Faktormodelle): Jedes Modell, das zukünftige Renditen (“Expected Returns”) vorhersagt, kann ein Faktor/Rankingmaß sein. Die prognostizierte zukünftige Rendite (bzw. die Ausprägungen der zugrunde liegenden Risikofaktoren) ist jetzt dieser Faktor und kann zum Ranking der Aktiengrundgesamtheit verwendet werden. Sie können prinzipiell jedes noch so komplizierte Preismodell in ein Ranking umwandeln.\nPreisbasierte Faktoren (Technische Indikatoren): Preisbasierte Faktoren nehmen Informationen über den historischen Preis jeder Aktie und verwenden sie, um den Faktorwert zu generieren. Beispiele dafür könnten 30-Tage-Momentum oder einfache Volatilitätsmaße sein.\nReversion vs. Momentum: Momentum Faktoren wetten darauf, dass Preise, wenn sie sich einmal in eine Richtung bewegen, dies auch weiterhin tun werden. Reversal Faktoren wetten auf das Gegenteil. Beides sind zulässige Modelle für unterschiedliche Zeithorizonte und Assetklassen, und es ist wichtig zu untersuchen, ob das zugrunde liegende Aktienkursverhalten auf Momentum oder Reversion beruht.\nFundamentale Faktoren: Fundamentalwerte enthalten Informationen (z.B. aus Bilanzen), die mit Fakten über ein Unternehmen verknüpft sind, so dass sie in vielerlei Hinsicht robuster sein können als vergangene Preise.\nThe Factor “Arms Race”: Letztlich ist die Entwicklung von Ranking-Faktoren mit Prognosegüte wie ein “Wettrüsten”, bei dem man versucht, gegenüber seinen Wettbewerbern immer einen Schritt voraus zu sein. Neue Faktoren haben nur eine begrenzte Lebensdauer, sie werden mit der Zeit zunehmend in Märkten eingepreist und verlieren somit ihre Pognosequalität. Daher ist es wichtig, ständig festzustellen, wie stark die Prognosegüte verwendeter Faktoren abnimmt und zu prüfen, welche neuen Faktoren an ihre Stelle treten könnten.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen des Factor Investings</span>"
    ]
  },
  {
    "objectID": "kapitel1.html#zusätzliche-überlegungen",
    "href": "kapitel1.html#zusätzliche-überlegungen",
    "title": "2  Grundlagen des Factor Investings",
    "section": "2.5 Zusätzliche Überlegungen",
    "text": "2.5 Zusätzliche Überlegungen\n\n2.5.1 Rebalancing-Frequenz\nJedes Rankingsystem wird Renditen über einen etwas anderen Zeitraum vorhersagen können. Ein preisbasiertes Mean Reversion Maß kann Renditen über einige Tage vorhersagen, während ein fundamentalwertbasiertes Faktormodell eher auf die Prognose längerfristiger Renditen ausgerichtet ist. Es ist wichtig den Zeitrahmen zu bestimmen, über den ein ausgewähltes Modell prädiktiv sein soll, und dies statistisch zu überprüfen bevor die Strategie ausgeführt wird. Bei der Bestimmung der optimalen Portfoliohaltedauer (Rebalancing Frequenz) gilt es natürlich auch wieder, overfitting zu vermeiden! Der optimale Horizont für die Renditeprognose (und damit Haltperiode) auf Basis von In-Sample Daten muss nicht optimal für Out-of-Sample Daten sein!\nSobald Sie den Zeitrahmen festgelegt haben für den Ihr Ranking-Schema prädiktiv ist, versuchen Sie, in etwa in dieser Frequenz das Portfolio umzuschichten, um damit die Prognosegüte des Modells voll ausnutzen zu können und Transaktionskosten zu sparen.\n\n\n2.5.2 Kapitalkapazität und Transaktionskosten\nJede Strategie besitzt ein Minimum und ein Maximum an investiertem Kapital, um profitabel zu sein. Insbesondere wenn zu viel Kapital in eine Strategie fließt, wird die der Strategie zugrunde liegende Marktineffizienz schnell eliminiert und die Strategie damit unprofitabel.\nDer Handel mit vielen Aktien verursacht hohe Transaktionskosten. Angenommen die Portfoliostrategie umfasst \\(1000\\) Aktien, dann entstehen pro Portfolioumschichtung Kosten in Höhe von Tausenden von Dollar/Euro. Die eingesetzte Kapitalbasis muss daher so hoch sein, dass die Transaktionskosten nur einen kleinen Prozentsatz der mit der Strategie erwirtschafteten Rendite ausmachen. Angenommen Sie setzen \\(100.000\\) Dollar ein und verdienen \\(1\\%\\) pro Monat, dann würden bereits \\(1000\\) Dollar an Transaktionskosten pro Monat Ihre gesamte Rendite eliminieren. Sie müssten die Strategie mit Millionen von Dollar betreiben, wenn sie mit \\(1000\\) Aktien profitabel sein soll.\nDie Mindestkapazität einer Portfoliostrategie ist daher generell eher hoch und hängt weitgehend von der Anzahl der gehandelten Aktien ab. Die maximale Kapazität ist meistens auch recht hoch, da Hunderte von Millionen Dollar in Long-Short Aktienstrategien investiert werden können ohne dass die Strategie ihre Prognosegüte verliert. Dies liegt daran, dass die Strategie relativ selten eine Portfolioumschichtung erfordert und das gesamte Dollarvolumen durch die Anzahl der gehandelten Aktien geteilt wird. Wenn Sie also Ihr gesamtes Portfolio von 100.000.000 Dollar investiert in 1000 Aktien jeden Monat “rebalancen”, dann halten und handeln sie im Durchschnitt nur 100.000 Dollar pro Monat in jeder Aktie, was in der Regel nicht ausreicht, um für die meisten Aktien einen signifikanten Marktanteil zu erreichen.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen des Factor Investings</span>"
    ]
  },
  {
    "objectID": "kapitel2.html",
    "href": "kapitel2.html",
    "title": "3  Preisbasierte Faktoren: Momentum und Reversal",
    "section": "",
    "text": "3.1 Hintergrund und Motivation\nBei Momentum bzw. Reversal handelt es sich um Trendfolge- bzw. Trendumkehr-Strategien. Der zugrundeliegende Faktor ist dabei die Aktienrendite gemessen über einen vergangenen Zeitraum. Bei Momentum basiert die Wette darauf dass zumindest kurzfristig Past Winner auch Future Winner sind, bzw. Past Loser auch Future Loser bleiben. Bei Reversal ist die Wette genau umgekehrt, Winner werden zu Losern und Loser zu Winnern.\nDer Momentum-Effekt wurde in der im Jahr 1993 erschienenen Studie (Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency, Journal of Finance, S. 65-91) von Jegadeesh und Titman erstmals umfangreich quantitativ belegt. Die Autoren zeigten für den Zeitraum von 1965 bis 1989, dass eine Handelsstrategie, welche die 10% der stärksten (schwächsten) Aktien kauft (leerverkauft), über einen Zeitraum von 3 bis 12 Monaten deutlich positive Renditen erzielt. Konkret berechnen die Autoren für eine jeweils 6-monatige Ranking- und Holding-Periode eine jährliche Überrendite von 12,01% vor Transaktionskosten. Eine lesenswerte Zusammenfassung der umfangreichen Momentum-Literatur gibt Gränitz (2014, S. 12-39).\nDas Phänomen der kurzfristigen Renditeumkehr, der sogenannte Short-Term Reversal (STR) Effekt, ist ein am Aktienmarkt seit mehr als 40 Jahren etabliertes Phänomen, das sich als robust und als ökonomisch und statistisch signifikant erwiesen hat. Jegadeesh (1990) zum Beispiel dokumentiert Gewinne von etwa 2% pro Monat über den Zeitraum 1934-1987 mit einer Umkehrstrategie, die Aktien auf der Grundlage ihrer Vormonatsrenditen kauft und verkauft und sie dann für einen Monat hält. Diese Gewinne lassen sich nicht durch direkte Transaktionskosten erklären. Weitere wichtige Arbeiten zum STR Effekt liefern Lehmann (1990), Lo und MacKinlay (1990), Nagel (2012), und Da, Liu und Schaumburg (2014).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preisbasierte Faktoren: Momentum und Reversal</span>"
    ]
  },
  {
    "objectID": "kapitel2.html#beginn-der-fallstudie",
    "href": "kapitel2.html#beginn-der-fallstudie",
    "title": "3  Preisbasierte Faktoren: Momentum und Reversal",
    "section": "3.2 Beginn der Fallstudie",
    "text": "3.2 Beginn der Fallstudie\n\n\nCode\n# import of necessary libaries\nimport numpy as np\nimport pandas as pd\n\n\nFür unsere beispielhafte Implementierung einer faktorbasierten Momentum bzw. Reversal Strategie wählen wir als Anlageuniversum die Aktien des S&P500. Wir laden Preishistorien für alle Indexmitglieder für den Zeitraum 3.1.2017 - 14.11.2019 aus Yahoo Finance.\n\n\nCode\ncd \"C:\\Users\\Galina\\Documents\\Thomas\\Python Projekte\\examplefiles\"\n\n\nC:\\Users\\Galina\\Documents\\Thomas\\Python Projekte\\examplefiles\n\n\n\n\nCode\n# load file from disk\npx = pd.read_csv('s&p_500_15112019.csv', \n                   parse_dates=True, index_col=0)\npx = px.asfreq('B').fillna(method='pad')\n\n\n\n\nCode\npx.tail(2)\n\n\n\n\n\n\n\n\n\nABT\nABBV\nABMD\nACN\nATVI\nADBE\nAMD\nAAP\nAES\nAMG\n...\nWLTW\nWYNN\nXEL\nXRX\nXLNX\nXYL\nYUM\nZBH\nZION\nZTS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-11-13\n84.220001\n86.660004\n220.009995\n194.50\n52.330002\n293.540009\n37.520000\n158.580002\n18.15\n84.690002\n...\n187.619995\n118.839996\n61.639999\n37.599998\n93.290001\n75.769997\n98.379997\n144.369995\n49.630001\n116.449997\n\n\n2019-11-14\n84.120003\n87.629997\n220.949997\n196.25\n52.700001\n294.529999\n38.349998\n156.830002\n18.41\n84.260002\n...\n189.750000\n118.680000\n61.299999\n38.650002\n93.150002\n76.510002\n98.110001\n145.029999\n49.500000\n116.250000\n\n\n\n\n2 rows × 502 columns\n\n\n\n\n3.2.1 Implementierung 1: gleichgewichtete Dezil-Portfolios\nFaktorstrategien können auf unterschiedliche Arten implementiert werden. Ein Standardverfahren ist es Aktien nach den Ausprägungen eines Faktors (hier: vergangene Renditen) in Dezile zu sortieren. Um die Strategie marktneutral zu halten wird in die Aktien des Dezil’s 10 “Long gegangen”, und das Dezil 1 wird “geshortet”. Die Strategie ist also ein Long-Short (Dezil 10-1) Ansatz. Die Aktien innerhalb der Dezile werden dabei am Anfang gleichgewichtet. Nach einer festgelegten Halteperiode kommt es zu einem Rebalancing (Neubestimmung der Dezile und Rückführung der Positionen auf eine Gleichgewichtung).\nLassen Sie uns beginnen und die Aktien des S&P500 an jedem Handelstag anhand ihrer vergangenen 20-Tagesrendite in Gruppen sortieren. Zunächst berechnen wir für jeden Tag t die diskrete Rendite für den Zeitraum von t-1 bis t-21. Dies geschieht durch px.shift(1).pct_change(20). Wir speichern die Renditen im DataFrame ret_df. Dann bringen wir die Aktien über die Methode rank(axis=1, pct=True) in eine Perzentil-Rankordnung gemäß der vergangenen 20-Tagesrendite. Aus den Perzentilen können wir Dezile generieren indem wir mit 10 multiplizieren (mul(10)) und dann auf die nächste ganze Zahl aufrunden (np.ceil). Wollen wir Aktien alternativ in Quintile sortieren, multiplizieren wir einfach mit 5 (mul(5)). Wichtig: durch die Festlegung ascending=False werden Aktien mit den höchsten (geringsten) Renditen in Gruppe 1 (10) sortiert. Äquivalent werden bei ascending=True die Past Winner in Gruppe 10 und die Past Loser in Gruppe 1 sortiert.\n\n\nCode\nret_df = px.shift(1).pct_change(20)\nrank_df = np.ceil(ret_df.rank(axis=1, pct=True, ascending=False).mul(10))\nrank_df.tail(5)\n\n\n\n\n\n\n\n\n\nABT\nABBV\nABMD\nACN\nATVI\nADBE\nAMD\nAAP\nAES\nAMG\n...\nWLTW\nWYNN\nXEL\nXRX\nXLNX\nXYL\nYUM\nZBH\nZION\nZTS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-11-08\n6.0\n3.0\n1.0\n6.0\n7.0\n5.0\n1.0\n4.0\n3.0\n1.0\n...\n8.0\n1.0\n10.0\n1.0\n6.0\n7.0\n10.0\n5.0\n2.0\n9.0\n\n\n2019-11-11\n5.0\n1.0\n1.0\n6.0\n8.0\n5.0\n1.0\n5.0\n3.0\n2.0\n...\n8.0\n2.0\n9.0\n1.0\n7.0\n7.0\n10.0\n5.0\n1.0\n10.0\n\n\n2019-11-12\n5.0\n1.0\n1.0\n6.0\n9.0\n5.0\n1.0\n4.0\n2.0\n1.0\n...\n8.0\n4.0\n9.0\n1.0\n8.0\n8.0\n10.0\n5.0\n1.0\n10.0\n\n\n2019-11-13\n6.0\n1.0\n1.0\n6.0\n10.0\n5.0\n1.0\n8.0\n2.0\n1.0\n...\n7.0\n5.0\n9.0\n1.0\n8.0\n8.0\n10.0\n5.0\n2.0\n10.0\n\n\n2019-11-14\n6.0\n1.0\n1.0\n5.0\n10.0\n3.0\n1.0\n9.0\n2.0\n2.0\n...\n7.0\n5.0\n9.0\n1.0\n8.0\n9.0\n10.0\n4.0\n2.0\n10.0\n\n\n\n\n5 rows × 502 columns\n\n\n\nWir transformieren das DataFrame rank_df nun in ein DataFrame mit Positionsindikatoren: -1 für eine Short Position in Dezil 1 Aktien, 0 für eine Flat (d.h. keine) Position in Aktien der Dezile 2 bis 9, und 1 für eine Long Position in Dezil 10 Aktien.\nBeachten Sie: Da wir Long in Dezil 10 und Short in Dezil 1 gehen, implementieren wir durch die Wahl von ascending=True eine Momentum Strategie, und durch ascending=False eine Reversal Strategie!\n\n\nCode\nfor col in rank_df.columns:\n    rank_df[col] = np.where(rank_df[col]&gt;9.0, 1, np.where(rank_df[col]&lt;2.0, -1, 0))\n\nrank_df.tail(5)\n\n\n\n\n\n\n\n\n\nABT\nABBV\nABMD\nACN\nATVI\nADBE\nAMD\nAAP\nAES\nAMG\n...\nWLTW\nWYNN\nXEL\nXRX\nXLNX\nXYL\nYUM\nZBH\nZION\nZTS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-11-08\n0\n0\n-1\n0\n0\n0\n-1\n0\n0\n-1\n...\n0\n-1\n1\n-1\n0\n0\n1\n0\n0\n0\n\n\n2019-11-11\n0\n-1\n-1\n0\n0\n0\n-1\n0\n0\n0\n...\n0\n0\n0\n-1\n0\n0\n1\n0\n-1\n1\n\n\n2019-11-12\n0\n-1\n-1\n0\n0\n0\n-1\n0\n0\n-1\n...\n0\n0\n0\n-1\n0\n0\n1\n0\n-1\n1\n\n\n2019-11-13\n0\n-1\n-1\n0\n1\n0\n-1\n0\n0\n-1\n...\n0\n0\n0\n-1\n0\n0\n1\n0\n0\n1\n\n\n2019-11-14\n0\n-1\n-1\n0\n1\n0\n-1\n0\n0\n0\n...\n0\n0\n0\n-1\n0\n0\n1\n0\n0\n1\n\n\n\n\n5 rows × 502 columns\n\n\n\nFühren wir nun alles zusammen in der Funktion weights_dc_equal. Diese Funktion erfordert als Eingabe ein DataFrame mit Aktienpreisen (price), die Angabe des Zeitraums (lookback) für die Berechnung der vergangenen diskreten Rendite als Ranking-Faktor, und die Anzahl der Tage (lag) die bis zur Implementierung der Strategie vergehen (sollen).\n\n\nCode\ndef weights_dc_equal(price, lookback, lag):\n    ret_df = price.shift(lag).pct_change(lookback)\n    # Important: False = Reversal; True = Momentum!!\n    rank_df = np.ceil(ret_df.rank(axis=1, pct=True, ascending=False).mul(10))\n    for col in rank_df.columns:\n        rank_df[col] = np.where(rank_df[col]&gt;9, 1, np.where(rank_df[col]&lt;2, -1, 0))\n    \n    return rank_df\n\n\nZudem definieren wir zwei Lambda Funktionen:\n1. \"compound\" berechnet aus dem Eingabe-Array x kumulative Mehr-Tagesrenditen;\n2. \"daily_sr\" berechnet aus einem Array von Tagesrenditen die tägliche Sharpe-Ratio;\n\n\nCode\n# cumulative returns\ncompound = lambda x: (1 + x).prod() - 1\n\n# daily Sharpe Ratio\ndaily_sr = lambda x: x.mean() / x.std()\n\n\nIm folgenden berechnen wir zunächst für jeden Handelstag die nicht-normierten Aktiengewichte (-1: Short; 0: Flat; 1: Long) durch Anwendung unserer Funktion weights_dc_equal. Das resultierende DataFrame nennen wir port. Dann legen wir die Anzahl der Handelstage (hold) fest, für die wir das Portfolio halten wollen ohne die Gewichte umzuschichten.\nIm letzten Schritt reduzieren wir die Zeitfrequenz von port auf die Länge (hold) der gewählten Portfoliohalteperiode, setzen die Gewichte auf die Werte die zu Beginn der Halteperiode gelten (über .first()), und wählen .shift(1), da in t nur die Gewichte die zum Zeitpunkt t-1 bekannt sind implementiert werden können.\n\n\nCode\nhold = 5\nfreq = '%dB' % hold # holding period \nport = weights_dc_equal(px, 20, lag=1) # security weights at business day freq\nport = port.shift(1).resample(freq).first() # time series with 'freq' as frequency\n\n\nDann berechnen wir die kumulativen Renditen jeder Aktie für die gewählte Portfoliohaltedauer. Hierzu verwenden wir die vorher definierte Lambda-Funktion compound.\n\n\nCode\n# calculation of daily security returns\ndaily_rets = px.pct_change()\n    \n# calculation of portfolio returns, shift(1) for implementation lag\nreturns = daily_rets.resample(freq).apply(compound)\n\n\nDanach multiplizieren wir für jede der einzelnen Halteperioden die kumulativen Aktienrenditen mit den Positionsindikatoren (-1, 0, 1), summieren die Produkte über alle Aktien auf, und teilen durch die Summe aller offen (Long und Short) Positionen um eine gleichgewichtete Portfoliorendite zu bekommen. Die resultierende Zeitreihe der kumulativen Portfoliorenditen nennen wir portf_rets. Sie hat dieselbe Zeitfrequenz wie freq, die Portfoliohaltedauer in Anzahl Handelstage. Wenden wir unsere Lambda-Funktion daily_srauf die Zeitreihe der Portfoliorenditen an und skalieren mit np.sqrt(252/hold) erhalten wir die annualisierte Sharpe-Ratio der Strategie.\n\n\nCode\n# security returns * position direction (-1, 0, 1)\nportf = np.multiply(port, returns) \n# summing position returns divided by number of positions\nportf_rets = portf.sum(axis = 1)/(portf != 0).sum(axis =1)\n\n\nLassen Sie uns nun die obigen Schritte in der Strategie-Funktion strat_dc_equal zusammenfassen. Wir benötigen ein DataFrame (prices) mit Aktienkursen, die Länge der lookback (lb) Periode zur Berechnung des Preisfaktors, und die Länge der Portfoliohaltedauer (hold).\n\n\nCode\n# strategy function\ndef strat_dc_equal(prices, lb, hold):\n    # calculation of security weights: (-1: short, 0: no, 1: long)\n    freq = '%dB' % hold # holding period \n    port = weights_dc_equal(prices, lb, lag=0) # security weights at business day freq\n    \n    # calculation of daily security returns\n    daily_rets = prices.pct_change()\n    \n    # calculation of portfolio returns, shift(1) for implementation lag\n    port = port.shift(1).resample(freq).first() # time series with 'freq' as frequency\n    returns = daily_rets.resample(freq).apply(compound)\n    portf = np.multiply(port, returns) # security returns * position direction (-1, 0, 1)\n    # summing position returns divided by number of positions\n    portf_rets = portf.sum(axis = 1)/(portf != 0).sum(axis =1)\n    \n    return portf_rets, daily_sr(portf_rets) * np.sqrt(252/hold)\n\n\nFühren wir die Strategie nun beispielhaft mit einer Halteperiode von einem Tag aus. Die Aktien des S&P500 werden anhand ihrer vergangenen 5-Tagesrendite sortiert. Da in der Funktion weights_dc_equal ascending=False festgelegt ist, gehen wir also Long in die extremen 10% der vergangenen 5-Tages Loser und Short in die extremen 10% der vergangenen 5-Tages Winner. Wir wetten also auf ein Short-Term Reversal!\n\n\nCode\n_, sharpe_equal = strat_dc_equal(px,5,1)\n\n\n\n\nCode\nsharpe_equal\n\n\n0.3033419609996148\n\n\n\n\n3.2.2 Implementierung 2: Marktwert-gewichtete Dezil-Portfolios\nAlternativ zur Gleichgewichtung bei Portfoliobildung können die Aktien in den beiden Portfolios (Dezil 10 und 1) auch Marktwert-gewichtet (value-weighted) werden. Hierzu benötigt man die Informationen zur täglichen Marktkapitalisierung (Schlusskurs * Anzahl emittierter Aktien) einer Aktie. Zur Veranschaulichung des Konzepts und der Implementierung verwenden wir im folgenden nur Daten zu Kursen und zur Marktkapitalisierung der S&P500 Aktien für einen Monat (18.11.2019 - 17.12.2019). Wir generieren aus dem Excel File “SnP500 1monat Schlusskurse und MarketCaps” zwei DataFrames: df.px mit den Schlusskursen und df.market_cap mit der täglichen Marktkapitalisierung (in Dollar) der S&P500 Aktien.\n\n\nCode\nxlsx = pd.ExcelFile('SnP500 1monat Schlusskurse und MarketCaps.xlsx')\n\ndf = pd.read_excel(xlsx, 'Sheet3', header=[0,1], index_col=0)\ndf = df.swaplevel(0,1, axis=1).sort_index(axis=1, level=0)\ndf = df.rename(columns={'px_last(dates=range(-1m,0d))': 'px', 'market_cap(dates=range(-1m,0d))': 'market_cap'})\n\n# giving names to the index/column labels\ndf.columns.names = ['var', 'ticker']\ndf.index.name = 'date'\n\n# dropping the last 10 strings in the ticker labels\ndrop_last_letters = lambda x: x[0:-10]\ndf.columns.set_levels(df.columns.levels[1].map(drop_last_letters),level=1,inplace=True)\n\n# setting frequency to business days\ndf = df.asfreq('B').ffill()\n\n\n\n\nCode\ndf.px.head(3)\n\n\n\n\n\n\n\n\nticker\nA\nAAL\nAAP\nAAPL\nABBV\nABC\nABMD\nABT\nACN\nADBE\n...\nXEL\nXLNX\nXOM\nXRAY\nXRX\nXYL\nYUM\nZBH\nZION\nZTS\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-11-18\n77.94\n28.60\n163.67\n267.10\n88.73\n87.77\n178.09\n84.26\n198.02\n297.16\n...\n60.88\n92.38\n68.52\n56.27\n39.30\n77.31\n98.28\n144.99\n49.305\n118.80\n\n\n2019-11-19\n79.29\n29.29\n161.98\n266.29\n89.02\n87.81\n184.34\n84.29\n198.34\n300.60\n...\n61.32\n92.80\n67.82\n56.39\n38.69\n78.00\n98.08\n144.54\n49.300\n120.33\n\n\n2019-11-20\n78.52\n28.23\n161.74\n263.19\n87.20\n87.55\n185.25\n83.89\n197.71\n300.10\n...\n61.69\n91.01\n68.03\n56.63\n38.30\n77.47\n98.12\n142.57\n48.885\n119.56\n\n\n\n\n3 rows × 505 columns\n\n\n\n\n\nCode\ndf.market_cap.head(3)\n\n\n\n\n\n\n\n\nticker\nA\nAAL\nAAP\nAAPL\nABBV\nABC\nABMD\nABT\nACN\nADBE\n...\nXEL\nXLNX\nXOM\nXRAY\nXRX\nXYL\nYUM\nZBH\nZION\nZTS\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-11-18\n2.411991e+10\n1.252846e+10\n1.133569e+10\n1.186796e+12\n1.312158e+11\n1.828473e+10\n8.041673e+09\n1.490101e+11\n1.259452e+11\n1.438488e+11\n...\n3.192475e+10\n2.323056e+10\n2.899154e+11\n1.251522e+10\n8.496199e+09\n1.392190e+10\n2.972598e+10\n2.982218e+10\n8.404721e+09\n5.658435e+10\n\n\n2019-11-19\n2.453769e+10\n1.283072e+10\n1.121864e+10\n1.183197e+12\n1.316447e+11\n1.829306e+10\n8.323893e+09\n1.490631e+11\n1.261487e+11\n1.455140e+11\n...\n3.215548e+10\n2.333617e+10\n2.869536e+11\n1.254191e+10\n8.364324e+09\n1.404615e+10\n2.966549e+10\n2.972962e+10\n8.403869e+09\n5.731309e+10\n\n\n2019-11-20\n2.429940e+10\n1.236638e+10\n1.120201e+10\n1.169423e+12\n1.289532e+11\n1.802849e+10\n8.364984e+09\n1.483557e+11\n1.257480e+11\n1.452720e+11\n...\n3.234950e+10\n2.288605e+10\n2.878421e+11\n1.259529e+10\n8.280010e+09\n1.395071e+10\n2.967759e+10\n2.932442e+10\n8.333126e+09\n5.694634e+10\n\n\n\n\n3 rows × 505 columns\n\n\n\nNachdem wir wie oben unser DataFrame rank_df mit den Positionsindikatoren (-1: Short; 0: Flat; 1: Long) erstellt haben, generieren wir daraus ein Dataframe ranks_long mit Positionsindikatoren (Einsen und Nullen) für Long Positionen und ein separates (ranks_short) für Short Positionen. Wir gewichten die “Einsen” mit der aktuellen Marktkapitalisierung der Aktie und normieren (teilen) durch die Summe der Marktkapitalsierungen aller Aktien in jedem der beiden Portfolios. Im Ergebnis erhalten wir ein DataFrame mit den täglichen Marktwert-basierten Gewichten für das Long Portfolio und eins für das Short Portfolio. Fassen wir die einzelnen Schritte in der Funktion weights_dc_value zusammen.\n\n\nCode\n# value weights function\ndef weights_dc_value(df, lookback, lag):\n    ret_df = df.px.shift(lag).pct_change(lookback)\n    # Important: False = Reversal; True = Momentum!\n    rank_df = np.ceil(ret_df.rank(axis=1, pct=True, ascending=False).mul(10))\n    for col in rank_df.columns:\n        rank_df[col] = np.where(rank_df[col]&gt;9, 1, np.where(rank_df[col]&lt;2, -1, 0))\n \n    # weights separately for the long and the short leg\n    ranks_short = rank_df.replace(1, 0)\n    ranks_long = rank_df.replace(-1, 0)\n    short_vranks = np.multiply(ranks_short, df.market_cap.shift())\n    long_vranks = np.multiply(ranks_long, df.market_cap.shift())\n\n\n    weights_short = short_vranks.div(short_vranks.sum(axis =1), axis=0)\n    weights_long = long_vranks.div(long_vranks.sum(axis =1), axis=0)\n\n    return weights_long, weights_short\n\n\nWir berechnen nun für das Long und das Short Portfolio separat die täglichen Gewichte, und verwenden die Gewichte zum Zeitpunkt t-1 für eine Halteperiode die in t beginnt. Mit diesen Gewichten multiplizieren wir die kumulativen Renditen der Aktien über die Halteperiode um die Portfoliorendite der Halteperiode zu bekommen. Die Strategierendite der Halteperiode ist die Differenz zwischen der Halteperiodenrendite des Long und des Short Portfolios. Wir fassen diese Schritte in der Strategiefunktion strat_dc_value zusammen.\n\n\nCode\n# strategy function\ndef strat_dc_value(df, lb, hold):\n    # calculation of security value-weights\n    freq = '%dB' % hold # holding period \n    weights_long, weights_short = weights_dc_value(df, lb, lag=0) # security weights at business day freq\n    \n    # calculation of daily security returns\n    daily_rets = df.px.pct_change()\n    returns = daily_rets.resample(freq).apply(compound)\n    \n    # calculation of portfolio returns, shift(1) for implementation lag\n    # first for the long leg\n    w_long = weights_long.shift(1).resample(freq).first() # time series with 'freq' as frequency\n    returns_long = np.multiply(w_long, returns).sum(axis=1) # security returns * position direction (-1, 0, 1)\n    \n    # and now for the short leg\n    w_short = weights_short.shift(1).resample(freq).first() # time series with 'freq' as frequency\n    returns_short = np.multiply(w_short, returns).sum(axis=1) # security returns * position direction (-1, 0, 1)\n    \n    # portfolio returns = returns_long - returns_short\n    portf_rets = returns_long - returns_short\n\n    return portf_rets, daily_sr(portf_rets) * np.sqrt(252/hold)\n\n\n\n\nCode\n_, sharpe_value = strat_dc_value(df,2,1)\n\n\n\n\nCode\nsharpe_value\n\n\n0.3585974000725961\n\n\n\n\n3.2.3 Implementierung 3: Gewichte nach Frazzini und Pedersen (2014)\nDer Ansatz von Frazzini und Pedersen (FP) (Betting Against Beta, Journal of Financial Economics, 2014, S. 1-25) sieht nicht die Bildung von Dezilen (oder Quintilen, etc.) vor. Im Gegensatz dazu wird in jede Aktie investiert, und zwar abhängig davon wie stark und in welche Richtung der Perzentil-Rang einer Aktie vom durchschnittlichen Rang aller Aktien abweicht. In Aktien mit negativen Abweichungen wird Short gegangen und in Aktien mit positiven Abweichungen wird eine Long Position aufgebaut. Das Gewicht jeder Aktie ergibt sich aus der Rangabweichung der Aktie vom mittleren Rang skaliert mit der (d.h. geteilt durch die) Hälfte der Summe der absoluten Rangabweichungen über alle Aktien. Das Long Portfolio hat damit wieder ein Gewicht von 1, und das Short Portfolio von -1.\nZur Implementierung der FP Gewichte erstellen wir das gewohnte (tägliche) DataFrame ranks mit den Perzentil-Rängen der Aktien gemäß ihrer Rendite über eine vergangene lookback Periode. Daraus erstellen wir ein neues DataFrame demeaned mit Rangabweichungen indem wir jeweils den Zeilenmittelwert (ranks.mean(axis=1)) der Ränge vom Rang einer Aktie abziehen. Zusätzlich enthält das DataFrame abs_demeaned die absoluten Rangabweichungen. Wir bekommen die finalen FP Gewichte (DataFrame weights) indem wir die Zeilenwerte von demeaned durch die Hälfte der korrespondierenden Zeilensumme von abs_demeaned teilen.\nFassen wir diese Schritte nun in der Gewichtsfunktion weights_fp zusammen. Alle weiteren Schritte erfolgen analog zur obigen Vorgehensweise. Die Strategiefunktion nenne ich strat_fp.\n\n\nCode\n# first, definition of the weight function\ndef weights_fp(price, lookback, lag):\n    # weights according to Frazzini and Pedersen (2014), equation (16), p. 9\n    ret = price.shift(lag).pct_change(lookback)\n    ranks = ret.rank(ascending=False, axis=1, pct=True) # percentile ranks\n    # important: False = Reversal; True = Momentum!\n    demeaned = ranks.sub(ranks.mean(axis=1), axis='index') # cross-sectional demeaned\n    abs_demeaned = abs(demeaned) \n    # demeaned percentile ranks normalized by 0.5 * cross-sectional sum of abs. demeaned weights\n    weights = demeaned.div(0.5 * abs_demeaned.sum(axis=1), axis='index')\n    return weights\n\n\n# strategie returns (main function)\ndef strat_fp(prices, lb, hold):\n    # portfolio weights\n    freq = '%dB' % hold # holding period in number of business days\n    port = weights_fp(prices, lb, lag=0) #weights for each business day\n    \n    # daily returns\n    daily_rets = prices.pct_change()\n    \n    # strategy returns\n    port = port.shift(1).resample(freq).first() # time series with holding period freq\n    returns = daily_rets.resample(freq).apply(compound)\n    port_rets = (port * returns).sum(axis=1) # weighted sum of security returns \n    \n    return port_rets, daily_sr(port_rets) * np.sqrt(252/hold)\n    \n\n\n\n\nCode\n_, sharpe_fp =strat_fp(px,5,1)\n\n\n\n\nCode\nsharpe_fp\n\n\n0.3799827125498262\n\n\n\n\n3.2.4 Optimierung der Lookback und Halteperioden\nLassen Sie uns zum Abschluss unsere Momentum bzw. Reversal Faktorstrategie optimieren indem wir mit einem “Brute Force” Ansatz nach der für die Backtestperiode optimalen Länge des Lookback Fensters und der Halteperiode suchen. Beachten Sie das Overfitting Risiko: die optimalen Längen ermittelt für die Backtestperiode werden mit hoher Wahrscheinlichkeit nicht optimal für eine Out-of-Sample Periode sein.\nWir erstellen dabei ein DataFrame ddf mit Zeilen für unterschiedliche Halteperioden (zwischen 1 und 10 Tagen mit Schrittlänge 2 Tagen) und Spalten für unterschiedliche Lookback Fensterlängen (zwischen 2 und 20 Tagen mit Schrittlänge 2 Tagen). Die Zellen des DataFrames enthalten die entsprechenden annualisierten Strategie Sharpe-Ratios.\n\n\nCode\nfrom collections import defaultdict\n\nlookbacks = range(2, 20, 2)\nholdings = range(1, 10, 2)\ndd = defaultdict(dict)\nfor lb in lookbacks:\n    for hold in holdings:\n        _, dd[lb][hold] = strat_dc_equal(px, lb, hold)\n        \nddf = pd.DataFrame(dd)\nddf.index.name = 'Holding Period'\nddf.columns.name = 'Lookback Period'\n\n\n\n\nCode\nddf\n\n\n\n\n\n\n\n\nLookback Period\n2\n4\n6\n8\n10\n12\n14\n16\n18\n\n\nHolding Period\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.586479\n0.622767\n0.263583\n0.386647\n0.478552\n0.770722\n0.368714\n0.612923\n0.384949\n\n\n3\n0.637111\n0.824397\n0.484336\n0.532511\n0.743088\n0.744577\n0.771689\n0.714077\n0.537295\n\n\n5\n0.081747\n0.035580\n0.576375\n0.997212\n0.839344\n0.582970\n0.430892\n0.803945\n0.662032\n\n\n7\n-0.543289\n-0.190297\n0.461219\n0.686883\n0.362453\n0.525375\n0.449932\n0.688291\n0.502556\n\n\n9\n0.236476\n0.585773\n0.339001\n0.389064\n0.438899\n0.555245\n0.645081\n0.401152\n0.500175\n\n\n\n\n\n\n\n\n\nCode\n# heatmap\nimport matplotlib.pyplot as plt\n%matplotlib notebook\n\ndef heatmap(df, cmap=plt.cm.gray_r):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    axim = ax.imshow(df.values, cmap=cmap, interpolation='nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    plt.colorbar(axim)\n    \nheatmap(ddf)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preisbasierte Faktoren: Momentum und Reversal</span>"
    ]
  },
  {
    "objectID": "kapitel3.html",
    "href": "kapitel3.html",
    "title": "4  Risikobasierte Faktoren: Total Risk",
    "section": "",
    "text": "4.1 Hintergrund und Motivation\nKlassische (“rationale”) Finance-Theory (z.B. das CAPM) besagt, dass in einem effizienten Markt riskantere Aktien mit höherem erwarteten Risiko auch höhere erwartete Renditen versprechen sollten um Investoren für das größere Risiko zu kompensieren. Es gibt aber signifikante empirische Evidenz die diesen fundamentalen positiven Risiko-Rendite Zusammenhang in Frage stellt. Das empirische Phänomen in dem risikoarme (d.h., Low Risk) Aktien eine höhere Kompensation pro Einheit Risiko (das sogenannte Alpha) versprechen als riskante (d.h., High Risk) Aktien wird als die Low Risk Anomalie (LRA) bezeichnet. Ang (2014) und Baker, Bradley und Wurgler (2011) geben eine sehr lesenswerte Einführung in die empirische Signifikanz und in potentielle Erklärungen zur Existenz der LRA. Es kann dabei gründsätzlich zwischen den folgenden Erklärungsansätzen unterschieden werden:\nAuf Basis der kumulativen Prospekttheorie (Barberis und Huang, 2008) wird argumentiert, dass insbesondere individuelle (“Retail”) Anleger kleine Chancen auf große Gewinne übergewichten. Sie besitzen demnach Lotteriepräferenzen. Infolgedessen bevorzugen sie Aktien mit positiver Schiefe (Skewness) in der Renditeverteilung. Diese Aktien sind dann aufgrund der erhöhten Nachfrage bei gegebenem Angebot überbewertet, was zu geringeren zukünftigen Renditen führt. Sind Aktienkursrisiko und Schiefe der Renditeverteilung positiv miteinander korreliert, können Lotteriepräferenzen die LRA erklären. Arbitragebeschränkungen in Form von Benchmarking und der Verzicht auf den Einsatz von Fremdkapital (siehe hierzu insbesondere Baker, Bradley und Wurgler, 2011) seitens institutioneller Investoren führen dazu, dass die LRA nicht “arbitriert” wird.\nDie zweite Gruppe von Erklärungen der LRA basiert auf sogenannten Market Frictions, also Abweichungen vom theoretischen Idealbild eines vollkommenen Marktes. Zu den Friktionen zählen insbesondere Transaktionskosten, Marktmikrostrukturverzerrungen wie der Bid-Ask Bounce oder eine kurzfristige Renditeumkehr (Mean Reversion).\nIm folgenden werden wir drei mögliche Wege im Form einer Faktorstrategie implementieren, mit dem Ziel eine potentiell vorhandene LRA auszunutzen. Im Kern geht es darum, High Risk (HR) und Low Risk (LR) Aktien zu identifizieren, und dann Long (Short) in die LR (HR) Aktien zu gehen. Die Strategierendite ergibt sich aus der Renditedifferenz zwischen dem Long und dem Short Portfolio. Wir messen die risikoadjustierte Strategierendite über die annualisierte Sharpe-Ratio.\nUnsere drei risikobasierten Faktorstrategien unterscheiden sich darin, wie wir das Aktienkursrisiko messen. Wir suchen also genau nach dem Risikomaß, das zukunftige Renditen am besten prognostiziert. Im ersten Fall verwenden wir die historische Renditestandardabweichung (Total Risk), im Zweiten die historische Standardabweichung der idiosynkratischen Renditen bzw. Renditeresiduen (Idiosyncratic Volatility), und schließlich das historische Aktienbeta (Beta).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Risikobasierte Faktoren: Total Risk</span>"
    ]
  },
  {
    "objectID": "kapitel3.html#hintergrund-und-motivation",
    "href": "kapitel3.html#hintergrund-und-motivation",
    "title": "4  Risikobasierte Faktoren: Total Risk",
    "section": "",
    "text": "Lotterie-Präferenzen irrationaler Investoren kombiniert mit Arbitragebeschränkungen\n\n\n\nAbweichungen vom Ideal eines friktionslosen Marktes (Market Frictions)\n\n\n\nErklärungen basierend auf Unsicherheit, Variance Beta, Earnings Surprises, etc.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Risikobasierte Faktoren: Total Risk</span>"
    ]
  },
  {
    "objectID": "kapitel3.html#beginn-der-fallstudie",
    "href": "kapitel3.html#beginn-der-fallstudie",
    "title": "4  Risikobasierte Faktoren: Total Risk",
    "section": "4.2 Beginn der Fallstudie",
    "text": "4.2 Beginn der Fallstudie\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n\nFür unsere beispielhafte Implementierung risikobasierter Faktorstrategien zur Ausnutzung der LRA wählen wir als Anlageuniversum die Aktien des S&P500 für einen täglichen Zeitraum vom 3.1.2017 bis zum 14.11.2019. Die Daten sind im File “s&p_500_15112019.csv” enthalten.\n\n\nCode\ncd \"C:\\Users\\Galina\\Documents\\Thomas\\Python Projekte\\examplefiles\"\n\n\nC:\\Users\\Galina\\Documents\\Thomas\\Python Projekte\\examplefiles\n\n\n\n\nCode\n# load file from disk\npx = pd.read_csv('s&p_500_15112019.csv', \n                   parse_dates=True, index_col=0)\npx = px.asfreq('B').fillna(method='pad')\n\n\n\n\nCode\npx.head(2)\n\n\n\n\n\n\n\n\n\nABT\nABBV\nABMD\nACN\nATVI\nADBE\nAMD\nAAP\nAES\nAMG\n...\nWLTW\nWYNN\nXEL\nXRX\nXLNX\nXYL\nYUM\nZBH\nZION\nZTS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-03\n36.866280\n54.056015\n112.360001\n110.738106\n35.941536\n103.480003\n11.43\n169.755737\n10.266887\n141.927734\n...\n118.256813\n81.712685\n37.324787\n25.328613\n55.914982\n47.835201\n60.320660\n101.052895\n40.778381\n52.556099\n\n\n2017-01-04\n37.158947\n54.818222\n115.739998\n111.004356\n36.647812\n104.139999\n11.43\n171.148819\n10.178833\n145.451904\n...\n119.715340\n84.347366\n37.490185\n26.284410\n55.507954\n48.548149\n60.540157\n101.981964\n41.363899\n53.066067\n\n\n\n\n2 rows × 502 columns\n\n\n\n\n4.2.1 Total Risk als Faktor\nLassen Sie uns beginnen und die Aktien des S&P500 an jedem Handelstag anhand ihrer vergangenen rollierenden historischen Renditestandardabweichung in Dezile sortieren. Zunächst berechnen wir für jeden Tag die diskrete Rendite. Dies geschieht durch px.pct_change(). Wir speichern die Renditen im DataFrame ret und berechnen über rolling(window, min_periods).std() ein neues DataFrame stdev mit rollierenden täglichen historischen Renditestandardabweichungen. Wir müssen die Länge für das rollierende Zeitfenster (Argument window) und die Mindestanzahl an Renditebeobachtungen (Argument min_periods) zur Berechnung der Standardabweichung festlegen.\n\n\n4.2.2 Implementierung 1: gleichgewichtete Dezil-Portfolios\nDanach bringen wir die Aktien über die Methode rank(axis=1, pct=True) in eine Perzentil-Rankordnung gemäß der vergangenen Renditestandardabweichung. Aus den Perzentilen können wir Dezile generieren indem wir mit 10 multiplizieren (mul(10)) und dann auf die nächste ganze Zahl aufrunden (np.ceil). Wollen wir Aktien alternativ in Quintile sortieren, multiplizieren wir einfach mit 5 (mul(5)). Wichtig: durch die Festlegung ascending=False werden Aktien mit den höchsten (geringsten) Standardabweichungen in Gruppe 1 (10) sortiert. Äquivalent werden bei ascending=True die riskanten Aktien in Gruppe 10 und die risikoarmen Aktien in Gruppe 1 sortiert.\n\n\nCode\n# calculation of daily returns\nret = px.pct_change().dropna(how='all')\n# calculation of std over rolling window\nstdev = ret.rolling(window=250, min_periods = 150).std() \n# important: ascending ='False': risky stocks in decile 1; riskless stocks in decile 10!\nrank_df = np.ceil(stdev.rank(axis=1, pct=True, ascending=False).mul(10))\nrank_df.tail(5)\n\n\n\n\n\n\n\n\n\nABT\nABBV\nABMD\nACN\nATVI\nADBE\nAMD\nAAP\nAES\nAMG\n...\nWLTW\nWYNN\nXEL\nXRX\nXLNX\nXYL\nYUM\nZBH\nZION\nZTS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-11-08\n8.0\n4.0\n1.0\n9.0\n2.0\n4.0\n1.0\n5.0\n8.0\n2.0\n...\n9.0\n1.0\n10.0\n2.0\n1.0\n6.0\n10.0\n7.0\n5.0\n7.0\n\n\n2019-11-11\n8.0\n4.0\n1.0\n9.0\n2.0\n4.0\n1.0\n5.0\n8.0\n2.0\n...\n9.0\n1.0\n10.0\n2.0\n1.0\n6.0\n10.0\n7.0\n5.0\n7.0\n\n\n2019-11-12\n8.0\n4.0\n1.0\n9.0\n2.0\n4.0\n1.0\n5.0\n8.0\n2.0\n...\n9.0\n1.0\n10.0\n2.0\n1.0\n6.0\n10.0\n7.0\n5.0\n7.0\n\n\n2019-11-13\n8.0\n4.0\n1.0\n9.0\n2.0\n4.0\n1.0\n5.0\n8.0\n2.0\n...\n9.0\n1.0\n10.0\n2.0\n1.0\n6.0\n10.0\n7.0\n5.0\n7.0\n\n\n2019-11-14\n8.0\n4.0\n1.0\n9.0\n2.0\n4.0\n1.0\n5.0\n8.0\n2.0\n...\n9.0\n1.0\n10.0\n2.0\n1.0\n6.0\n10.0\n7.0\n5.0\n7.0\n\n\n\n\n5 rows × 502 columns\n\n\n\nWir transformieren das DataFrame rank_df nun in ein DataFrame mit Positionsindikatoren: -1 für eine Short Position in Dezil 1 Aktien, 0 für eine Flat (d.h. keine) Position in Aktien der Dezile 2 bis 9, und 1 für eine Long Position in Dezil 10 Aktien.\nBeachten Sie: Da wir Long in Dezil 10 und Short in Dezil 1 gehen, implementieren wir durch die Wahl von ascending=True eine High-Risk Strategie, und durch ascending=False eine Low-Risk Strategie!\n\n\nCode\nfor col in rank_df.columns:\n    rank_df[col] = np.where(rank_df[col]&gt;9, 1, np.where(rank_df[col]&lt;2, -1, 0))\nrank_df.tail(5)\n\n\n\n\n\n\n\n\n\nABT\nABBV\nABMD\nACN\nATVI\nADBE\nAMD\nAAP\nAES\nAMG\n...\nWLTW\nWYNN\nXEL\nXRX\nXLNX\nXYL\nYUM\nZBH\nZION\nZTS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2019-11-08\n0\n0\n-1\n0\n0\n0\n-1\n0\n0\n0\n...\n0\n-1\n1\n0\n-1\n0\n1\n0\n0\n0\n\n\n2019-11-11\n0\n0\n-1\n0\n0\n0\n-1\n0\n0\n0\n...\n0\n-1\n1\n0\n-1\n0\n1\n0\n0\n0\n\n\n2019-11-12\n0\n0\n-1\n0\n0\n0\n-1\n0\n0\n0\n...\n0\n-1\n1\n0\n-1\n0\n1\n0\n0\n0\n\n\n2019-11-13\n0\n0\n-1\n0\n0\n0\n-1\n0\n0\n0\n...\n0\n-1\n1\n0\n-1\n0\n1\n0\n0\n0\n\n\n2019-11-14\n0\n0\n-1\n0\n0\n0\n-1\n0\n0\n0\n...\n0\n-1\n1\n0\n-1\n0\n1\n0\n0\n0\n\n\n\n\n5 rows × 502 columns\n\n\n\nFühren wir nun alles zusammen in der Funktion weights_dc_equal. Diese Funktion erfordert als Eingabe ein DataFrame mit Aktienpreisen (price), die Angabe des Zeitraums (window) für die Berechnung der historischen Renditestandardabweichung als Ranking-Faktor, die Minimum-Anzahl (min_periods) an Renditebeobachtungen, und die Anzahl der Tage (lag) die bis zur Implementierung der Strategie vergehen (sollen).\n\n\nCode\n# 'price' = DataFrame with daily stock prices \ndef weights_dc_equal(price, window=250, min_periods=50, lag=0):\n    # calculation of daily returns\n    ret = price.shift(lag).pct_change().dropna(how='all')\n    # calculation of std over rolling window\n    stdev = ret.rolling(window, min_periods = min_periods).std() \n    # important: ascending ='False': risky stocks in decile 1; riskless stocks in decile 10!\n    rank_df = np.ceil(stdev.rank(axis=1, pct=True, ascending=False).mul(10))\n    for col in rank_df.columns:\n        rank_df[col] = np.where(rank_df[col]&gt;9, 1, np.where(rank_df[col]&lt;2, -1, 0))\n    return rank_df\n\n\nZudem definieren wir wieder unsere zwei Lambda Funktionen:\n1. \"compound\" berechnet aus dem Eingabe-Array x kumulative Mehr-Tagesrenditen;\n2. \"daily_sr\" berechnet aus einem Array von Tagesrenditen die tägliche Sharpe-Ratio;\n\n\nCode\n# cumulative returns\ncompound = lambda x: (1 + x).prod() - 1\n\n# daily Sharpe Ratio\ndaily_sr = lambda x: x.mean() / x.std()\n\n\nIm folgenden berechnen wir zunächst für jeden Handelstag die nicht-normierten Aktiengewichte (-1: Short; 0: Flat; 1: Long) durch Anwendung unserer Funktion weights_dc_equal. Das resultierende DataFrame nennen wir port. Dann legen wir die Anzahl der Handelstage (hold) fest, für die wir das Portfolio halten wollen ohne die Gewichte umzuschichten.\nIm letzten Schritt reduzieren wir die Zeitfrequenz von port auf die Länge (hold) der gewählten Portfoliohalteperiode, setzen die Gewichte auf die Werte die zu Beginn der Halteperiode gelten (über .first()), und wählen .shift(1), da in t nur die Gewichte die zum Zeitpunkt t-1 bekannt sind implementiert werden können.\n\n\nCode\nhold = 21\nfreq = '%dB' % hold # holding period \nport = weights_dc_equal(px) # security weights at business day freq\n# calculation of portfolio returns\nport = port.shift(1).resample(freq).first() # time series with 'freq' as frequency\n\n\nDann berechnen wir die kumulativen Renditen jeder Aktie für die gewählte Portfoliohaltedauer. Hierzu verwenden wir die vorher definierte Lambda-Funktion compound.\n\n\nCode\n# calculation of daily security returns\ndaily_rets = px.pct_change().dropna(how='all')\n    \n# calculation of portfolio returns, shift(1) for implementation lag\nreturns = daily_rets.resample(freq).apply(compound)\n\n\nDanach multiplizieren wir für jede der einzelnen Halteperioden die kumulativen Aktienrenditen mit den Positionsindikatoren (-1, 0, 1), summieren die Produkte über alle Aktien auf, und teilen durch die Summe aller offen (Long und Short) Positionen um eine gleichgewichtete Portfoliorendite zu bekommen. Die resultierende Zeitreihe der kumulativen Portfoliorenditen nennen wir portf_rets. Sie hat dieselbe Zeitfrequenz wie freq, die Portfoliohaltedauer in Anzahl Handelstage. Wenden wir unsere Lambda-Funktion daily_srauf die Zeitreihe der Portfoliorenditen an und skalieren mit np.sqrt(252/hold) erhalten wir die annualisierte Sharpe-Ratio der Strategie.\n\n\nCode\n# security returns * position direction (-1, 0, 1)\nportf = np.multiply(port, returns) \n# summing position returns divided by number of positions\nportf_rets = portf.sum(axis = 1)/(portf != 0).sum(axis =1)\n\n\nLassen Sie uns nun die obigen Schritte in der Strategie-Funktion strat_dc_equal zusammenfassen. Wir benötigen ein DataFrame (prices) mit Aktienkursen, die Länge der lookback (window) Periode zur Berechnung der historischen Renditestandardabweichung (Rankingfaktor), und die Länge der Portfoliohaltedauer (hold).\n\n\nCode\n# strategy function\ndef strat_dc_equal(prices, window, hold):\n    # calculation of security weights: (-1: short, 0: no, 1: long)\n    # security weights at business day freq\n    port = weights_dc_equal(prices, window, min_periods=40, lag=0)\n    freq = '%dB' % hold # holding period \n    \n    # calculation of daily security returns\n    daily_rets = prices.pct_change().dropna(how='all')\n\n    # calculation of portfolio returns\n    port = port.shift(1).resample(freq).first() # time series with 'freq' as frequency\n    returns = daily_rets.resample(freq).apply(compound)\n    portf = np.multiply(port, returns) # security returns * position direction (-1, 0, 1)\n    # summing position returns divided by number of positions\n    portf_rets = portf.sum(axis = 1)/(portf != 0).sum(axis =1)\n    \n    return daily_sr(portf_rets) * np.sqrt(252/hold)\n\n\nFühren wir die Strategie nun beispielhaft mit einer Halteperiode von einem Monat (21 Handelstage) aus. Die Aktien des S&P500 werden anhand ihrer vergangenen 40-Tage Renditestandardabweichung sortiert. Da in der Funktion weights_dc_equal ascending=False festgelegt ist, gehen wir also Long in die extremen 10% der vergangenen Low-Risk Aktien und Short in die extremen 10% der vergangenen High-Risk Aktien. Wir implementieren also eine Low-Risk Strategie!\n\n\nCode\nsharpe_equal = strat_dc_equal(px,40,21)\n\n\n\n\nCode\nsharpe_equal\n\n\n0.6178071759738956\n\n\n\n\n4.2.3 Implementierung 2: Gewichte nach Frazzini und Pedersen (2014)\nZur Implementierung der FP Gewichte erstellen wir das gewohnte (tägliche) DataFrame ranks mit den Perzentil-Rängen der Aktien gemäß ihrer Renditestandardabweichung über eine vergangene lookback (window) Periode. Daraus erstellen wir ein neues DataFrame demeaned mit Rangabweichungen indem wir jeweils den Zeilenmittelwert (ranks.mean(axis=1)) der Ränge vom Rang einer Aktie abziehen. Zusätzlich enthält das DataFrame abs_demeaned die absoluten Rangabweichungen. Wir bekommen die finalen FP Gewichte (DataFrame weights) indem wir die Zeilenwerte von demeaned durch die Hälfte der korrespondierenden Zeilensumme von abs_demeaned teilen.\nFassen wir diese Schritte nun in der Gewichtsfunktion weights_fp zusammen. Alle weiteren Schritte erfolgen analog zur obigen Vorgehensweise. Die Strategiefunktion nenne ich strat_fp.\n\n\nCode\n# weights according to Frazzini and Pedersen (2014), equation (16), p. 9\ndef weights_fp(price, window=250, min_periods=40):\n    ret = price.pct_change().dropna(how='all')\n    # calculation of std over rolling window\n    stdev = ret.rolling(window, min_periods = min_periods).std() \n    \n    # important: ascending ='False': risky stocks in decile 1; riskless stocks in decile 10!\n    ranks = stdev.rank(ascending=False, axis=1, pct=True) # percentile ranks\n    demeaned = ranks.sub(ranks.mean(axis=1), axis='index') # cross-sectional demeaned\n    abs_demeaned = abs(demeaned) \n    # demeaned percentile ranks normalized by 0.5 * cross-sectional sum of abs. demeaned weights\n    weights = demeaned.div(0.5 * abs_demeaned.sum(axis=1), axis='index')\n    return weights\n\n# cumulative returns\ncompound = lambda x: (1 + x).prod() - 1\n\n# daily sharpe ratio\ndaily_sr = lambda x: x.mean() / x.std()\n\n# strategie returns (main function)\ndef strat_fp(prices, window, hold):\n    # portfolio weights\n    freq = '%dB' % hold # holding period in number of business days\n    port = weights_fp(prices, window) # weights for each business day\n    \n    # daily returns\n    daily_rets = prices.pct_change().dropna(how='all')\n    \n    # strategy returns; shift(1) for implementation lag\n    port = port.shift(1).resample(freq).first() # time series with holding period freq\n    returns = daily_rets.resample(freq).apply(compound)\n    port_rets = (port * returns).sum(axis=1) # weighted sum of security returns \n    \n    return daily_sr(port_rets) * np.sqrt(252/hold)\n    \n\n\n\n\nCode\nsharpe_fp = strat_fp(px,40,21)\n\n\n\n\nCode\nsharpe_fp\n\n\n0.6354001379129001\n\n\n\n\n4.2.4 Optimierung\n\n\nCode\nfrom collections import defaultdict\n\nlookbacks = range(40, 120, 20)\nholdings = range(20, 60, 5)\ndd = defaultdict(dict)\nfor lb in lookbacks:\n    for hold in holdings:\n        dd[lb][hold] = strat_fp(px, lb, hold)\n        \nddf = pd.DataFrame(dd)\nddf.index.name = 'Holding Period'\nddf.columns.name = 'Lookback Period'\n\n\n\n\nCode\nddf\n\n\n\n\n\n\n\n\nLookback Period\n40\n60\n80\n100\n\n\nHolding Period\n\n\n\n\n\n\n\n\n20\n0.547225\n0.535322\n0.566154\n0.486525\n\n\n25\n0.640504\n0.695662\n0.634289\n0.583816\n\n\n30\n0.565702\n0.610219\n0.584228\n0.522113\n\n\n35\n0.574164\n0.568566\n0.501421\n-0.133397\n\n\n40\n0.466834\n0.516816\n0.461860\n0.409636\n\n\n45\n0.711387\n0.706873\n0.602465\n0.578311\n\n\n50\n0.702979\n0.696906\n0.667072\n-0.112067\n\n\n55\n0.708629\n0.616248\n0.579317\n-0.114633\n\n\n\n\n\n\n\n\n\nCode\n# Heatmap zur Visualiserung\nimport matplotlib.pyplot as plt\n%matplotlib notebook\n\ndef heatmap(df, cmap=plt.cm.gray_r):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    axim = ax.imshow(df.values, cmap=cmap, interpolation='nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    plt.colorbar(axim)\n    \nheatmap(ddf)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Risikobasierte Faktoren: Total Risk</span>"
    ]
  },
  {
    "objectID": "kapitel4.html",
    "href": "kapitel4.html",
    "title": "5  Risikobasierte Faktoren: Idiosyncratic Volatility - IVOL",
    "section": "",
    "text": "5.1 Hintergrund und Motivation\nDas CAPM impliziert, dass das Gesamtrisiko (Renditevarianz) eines Wertpapiers aus zwei Summanden besteht, von denen der erste Summand die Renditeschwankungen des Marktes und der zweite Summand die wertpapierspezifischen Renditeschwankungen beinhaltet.\n\\[Var(R_{i,t}) = \\beta^{2}_{i} Var(R_{m,t}) + Var(\\epsilon_{i})\\]\n\\(\\epsilon_{i}\\) kennzeichnet dabei die Renditeresiduen des Wertpapiers \\(i\\), und \\(Var(\\epsilon_{i})\\) wird auch als idiosynkratische Varianz (unsystematisches Risiko) bezeichnet. Die Wurzel daraus ist die idiosynkratische Volatilität (IVOL). Verallgemeinert man den obigen Zusammenhang wird offensichtlich, dass sich IVOL auf Basis unterschiedlicher (Ein-) oder Mehrfaktormodelle berechnen läßt, immer als realisierte (empirische) Standardabweichung der sich aus der Faktorregression ergebenden Renditeresiduen.\nIn zwei einflussreichen Arbeiten dokumentieren Ang, Hodrick, Xing und Zhang (2006, 2009) einen negativen Zusammenhang zwischen der idiosynkratischen Volatilität und zukünftigen Aktienrenditen. In dem Maße, in dem die realisierte idiosynkratische Volatilität für die erwartete idiosynkratische Volatilität steht, ist dieses Ergebnis unerwartet (ein „Puzzle“), da traditionelle Asset-Pricing-Theorien (z.B. das CAPM) entweder keinen Zusammenhang zwischen der erwarteten idiosynkratischen Volatilität und erwarteten Renditen vorhersagen wenn Märkte vollständig und friktionslos sind und Anleger sich gut diversifizieren können, oder eine positive Beziehung prognostizieren unter der Annahme, dass Märkte unvollständig und mit Friktionen behaftet sind und Anleger schlecht diversifizierte Portfolios halten.\nEine bekannte Form der Low Risk Anomaly ist folglich das Idiosyncratic Volatility Puzzle (IVP). Hou und Loh (2016) untersuchen empirisch zahlreiche Erklärungsansätze für die Existenz des IVP.\nWir werden im folgenden eine Faktorstrategie mit IVOL als renditeprognostizierenden Faktor implementieren.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risikobasierte Faktoren: Idiosyncratic Volatility - IVOL</span>"
    ]
  },
  {
    "objectID": "kapitel4.html#beginn-der-fallstudie",
    "href": "kapitel4.html#beginn-der-fallstudie",
    "title": "5  Risikobasierte Faktoren: Idiosyncratic Volatility - IVOL",
    "section": "5.2 Beginn der Fallstudie",
    "text": "5.2 Beginn der Fallstudie\nWir beginnen mit dem Laden der notwendigen Pakete. Zur Durchführung von OLS Regressionen (mit einer Konstanten) importieren wir die Module linear_model und toolsaus dem Paket statsmodels.\n\n\nCode\nimport statsmodels.regression.linear_model as sm\nimport statsmodels.tools.tools as sm2\nimport numpy as np\nimport pandas as pd\n\n\nWir setzen die IVOL Faktorstrategie am Beispiel des S&P500 Universums um. Zunächst laden wir zwei Datensätze in Form von DataFrames, eines für die täglichen Aktienkurshistorien der S&P500 Mitglieder und eines für die Zeitreihe der Indexwerte des S&P500. Wir mergen beide Datensätze in dem DataFrame joined_df und berechnen diskrete Tagesrenditen.\n\n\nCode\n# load of data: df with S&P500 constituents and df with S&P500 index values\n%cd \"C:\\Users\\Galina\\Documents\\Thomas\\Python Projekte\\examplefiles\"\nconstituents = pd.read_csv('s&p_500_15112019.csv', \n                   parse_dates=True, index_col=0)\nindex = pd.read_csv('s&p_500_012000_102019.csv',parse_dates=True, index_col=0) \nsp_index = index['Adj Close']\nsp_index.name = 'SP_Index'\n\n# adjustment for different frequency of constituents and index\njoined_df = pd.merge(constituents, sp_index, how='inner', on='Date')\ndf_returns = joined_df.pct_change().dropna(how='all')        \n\n\nC:\\Users\\Galina\\Documents\\Thomas\\Python Projekte\\examplefiles\n\n\n\n\nCode\ndf_returns.head(5)\n\n\n\n\n\n\n\n\n\nABT\nABBV\nABMD\nACN\nATVI\nADBE\nAMD\nAAP\nAES\nAMG\n...\nWYNN\nXEL\nXRX\nXLNX\nXYL\nYUM\nZBH\nZION\nZTS\nSP_Index\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-04\n0.007939\n0.014100\n0.030082\n0.002404\n0.019651\n0.006378\n0.000000\n0.008206\n-0.008576\n0.024831\n...\n0.032243\n0.004431\n0.037736\n-0.007279\n0.014904\n0.003639\n0.009194\n0.014359\n0.009703\n0.005722\n\n\n2017-01-05\n0.008638\n0.007584\n-0.008035\n-0.014991\n0.015525\n0.016996\n-0.016623\n-0.000698\n-0.012976\n-0.012248\n...\n0.012849\n0.000000\n-0.004196\n-0.012108\n-0.009129\n0.003310\n0.006425\n-0.016210\n-0.003326\n-0.000771\n\n\n2017-01-06\n0.027204\n0.000314\n0.005313\n0.011392\n-0.000791\n0.022566\n0.007117\n-0.013090\n0.035933\n-0.002236\n...\n0.010827\n0.002941\n-0.015449\n0.019334\n-0.007010\n0.012097\n0.000095\n0.006498\n0.003152\n0.003517\n\n\n2017-01-09\n-0.000981\n0.006584\n0.014642\n-0.011178\n-0.005539\n0.002493\n0.015018\n-0.000589\n-0.024535\n-0.025943\n...\n0.003462\n-0.015152\n-0.005706\n0.000170\n-0.004236\n0.002794\n0.019436\n-0.010837\n-0.002773\n-0.003549\n\n\n2017-01-10\n0.013500\n-0.002180\n-0.041585\n0.000522\n0.018037\n-0.002855\n-0.004352\n0.002300\n-0.017346\n-0.001116\n...\n0.022426\n-0.000248\n0.008608\n-0.008974\n-0.004658\n0.005883\n0.062337\n0.015384\n-0.000371\n0.000000\n\n\n\n\n5 rows × 503 columns\n\n\n\n\n5.2.1 IVOL als Faktor\nWir berechnen die IVOL einer Aktie auf Basis eines Einfaktor-Modells (Single-Index Model), wobei die S&P Indexrendite den (Markt-) Faktor darstellt. Durch die Regression von Aktienrenditen auf die Indexrendite erhalten wir Residuen, deren Streuung (Standardabweichung) die idiosynkratische Volatilität (IVOL) einer Aktie symbolisiert. Wir berechnen die IVOL täglich auf Basis eines rollierenden Zeitfensters dessen Länge wir mit dem Argument window festlegen.\nLassen Sie uns zunächst eine Funktion idiovola_single() schreiben, die die rollierende IVOL für eine einzelne Aktie berechnet. Das Argument col bezeichnet hierbei die Historie der täglichen Aktienrenditen (y-Variable der Regression) und die Spalte ‘SP_Index’ des DataFrames df_returns enthält die x-Variable (Indexrenditen). Wichtig: Wollen wir IVOLs auf Basis von Multifaktoren-Modellen (z.B. dem Fama/French 3-Faktor Modell oder Erweiterungen hiervon) berechnen, muss df_returns entsprechende Zeitreihen der Faktorrenditen enthalten.\nÜber die Funktion add_constant fügen wir zum Vektor der Indexrenditen einen gleichlangen Vektor mit “Einsen”, die den Achsenabschnitt (Konstante der Regression) representieren, hinzu.\nFür jedes Zeitfenster der Länge window (von iStart bis iEnd) führen wir über sm.OLS(y-Variable, (Konstante, x-Variable)).fit() eine OLS Regression durch, berechnen die Standardabweichung der sich ergebenden Residuen (std_resids = IVOL) und speichern alle IVOLs in der Liste empty_list.\nDiese Liste wird anschließend in ein einspaltiges DataFrame idio_vola transformiert, wobei der Zeilenindex den Zeitpunkten (df_new['Date']) entspricht, für die jeweils die täglichen rollierenden IVOLs berechnet wurden. Beachten: Durch reset_index('Date') wird der Zeilenindex ‘Date’ des DataFrames df_returns in eine normale Spalte transformiert, die dann wiederum über pd.concat() mit Spalten anderer DataFrames verkettet werden kann.\n\n\nCode\n# function to calculate rolling IVOL for a df of stock and factor returns\n# first a function for a single stock\ndef idiovola_single(col, df_returns, window=250):\n    # col =y-variable; df_returns.SP_Index = x-variable\n    x1 = df_returns.SP_Index # adjustment necessary for multi-factors!\n    x2 = sm2.add_constant(x1)\n    empty_list = list()\n    # rolling window:[iStart,iEnd]\n    for iStart in range(0, len(df_returns)-window):\n        iEnd = iStart+window        \n        # Calculate regression residuals std\n        reg = sm.OLS(col[iStart:iEnd],x2[iStart:iEnd]).fit()\n        std_resids = np.std(reg.resid)\n        empty_list.append(std_resids)\n\n    idio_vola0 = pd.DataFrame(empty_list)\n    df_new = df_returns[window:len(df_returns)].reset_index('Date')\n    idio_vola = pd.concat([df_new['Date'], idio_vola0], axis=1)\n    idio_vola = idio_vola.set_index('Date')\n    return idio_vola\n\n\nNun schreiben wir eine Funktion, die rollierende IVOLs für eine Vielzahl von Aktien berechnet. Das DataFrame df_returns muss herbei zwingend eine Spalte mit der Bezeichnung ‘SP_Index’ enthalten, in der die Indexrenditen stehen. In den übrigen Spalten stehen die Tagesrenditen unseres Anlageuniversums.\nWir generieren durch den wiederholten Aufruf von idiovola_single() eine Liste dfs, die einspaltige DataFrames mit den jeweiligen IVOL Historien der Aktien enthält. Aus dieser Liste erzeugen wir das finale IVOL DataFrame df_idiovola.\n\n\nCode\n# now, the function for all stocks\ndef df_idiovola(df_returns):\n    dfs = []\n    for column in df_returns.columns:\n        dfs.append(idiovola_single(df_returns[column], df_returns))\n\n    df_idiovola = pd.concat(dfs, axis=1).sort_index()    \n    df_idiovola.columns = df_returns.columns\n    df_idiovola.drop('SP_Index', axis=1, inplace=True)\n    return df_idiovola\n\n\nBerechnen wir nun die Zeitreihe täglicher rollierender IVOLs für unser S&P500 Anlageuniversum für ein gleitendes Zeitfenster von 250 Handelstagen.\n\n\nCode\nIVOL = df_idiovola(df_returns)\nIVOL.head(5)\n\n\n\n\n\n\n\n\n\nABT\nABBV\nABMD\nACN\nATVI\nADBE\nAMD\nAAP\nAES\nAMG\n...\nWLTW\nWYNN\nXEL\nXRX\nXLNX\nXYL\nYUM\nZBH\nZION\nZTS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2018-01-02\n0.007889\n0.010438\n0.013948\n0.007202\n0.017943\n0.012580\n0.035296\n0.024660\n0.013050\n0.010132\n...\n0.007843\n0.016110\n0.006841\n0.011884\n0.012397\n0.009113\n0.007934\n0.011642\n0.012232\n0.008758\n\n\n2018-01-03\n0.008003\n0.010442\n0.013915\n0.007202\n0.017936\n0.012579\n0.035424\n0.024952\n0.013032\n0.010187\n...\n0.008065\n0.016213\n0.006856\n0.011713\n0.012366\n0.009132\n0.007945\n0.011709\n0.012272\n0.008780\n\n\n2018-01-04\n0.007992\n0.010448\n0.013912\n0.007141\n0.017909\n0.012548\n0.035490\n0.024955\n0.013011\n0.010199\n...\n0.008090\n0.016244\n0.006874\n0.011722\n0.012369\n0.009119\n0.007952\n0.011701\n0.012260\n0.008777\n\n\n2018-01-05\n0.007869\n0.010466\n0.013927\n0.007141\n0.017939\n0.012508\n0.035581\n0.025043\n0.012829\n0.010297\n...\n0.008099\n0.016241\n0.006895\n0.011673\n0.012360\n0.009090\n0.007943\n0.011704\n0.012261\n0.008778\n\n\n2018-01-08\n0.007874\n0.010471\n0.013879\n0.007121\n0.017956\n0.012503\n0.035621\n0.025045\n0.012749\n0.010214\n...\n0.008095\n0.016237\n0.006843\n0.011673\n0.012648\n0.009114\n0.007937\n0.011616\n0.012277\n0.008785\n\n\n\n\n5 rows × 502 columns\n\n\n\n\n\n5.2.2 Implementierung 1: gleichgewichtete Dezil-Portfolios\nZunächst bringen wir die Aktien über die Methode rank(axis=1, pct=True) in eine Perzentil-Rankordnung gemäß der vergangenen IVOL. Aus den Perzentilen können wir Dezile generieren indem wir mit 10 multiplizieren (mul(10)) und dann auf die nächste ganze Zahl aufrunden (np.ceil). Wollen wir Aktien alternativ in Quintile sortieren, multiplizieren wir einfach mit 5 (mul(5)). Wichtig: durch die Festlegung ascending=False werden Aktien mit den höchsten (geringsten) IVOL in Gruppe 1 (10) sortiert. Äquivalent werden bei ascending=True die riskanten Aktien in Gruppe 10 und die risikoarmen Aktien in Gruppe 1 sortiert.\nWir transformieren das DataFrame rank_df nun in ein DataFrame mit Positionsindikatoren: -1 für eine Short Position in Dezil 1 Aktien, 0 für eine Flat (d.h. keine) Position in Aktien der Dezile 2 bis 9, und 1 für eine Long Position in Dezil 10 Aktien.\nBeachten Sie: Da wir Long in Dezil 10 und Short in Dezil 1 gehen, implementieren wir durch die Wahl von ascending=True eine High-Risk Strategie, und durch ascending=False eine Low-Risk Strategie!\nWir führen nun alles zusammen in der Funktion weights_dc.\n\n\nCode\n# 'df_returns' = DataFrame with security daily returns and factor returns\ndef weights_dc(df_returns):\n    stdev = df_idiovola(df_returns)\n    # Important: False = long in low IVOL stocks\n    rank_df = np.ceil(stdev.rank(axis=1, pct=True, ascending=True).mul(10))\n    for col in rank_df.columns:\n        rank_df[col] = np.where(rank_df[col]&gt;9, 1, np.where(rank_df[col]&lt;2, -1, 0))\n    \n    return rank_df\n\n\nZudem definieren wir wieder unsere zwei Lambda Funktionen:\n1. \"compound\" berechnet aus dem Eingabe-Array x kumulative Mehr-Tagesrenditen;\n2. \"daily_sr\" berechnet aus einem Array von Tagesrenditen die tägliche Sharpe-Ratio;\n\n\nCode\n# cumulative returns\ncompound = lambda x: (1 + x).prod() - 1\n\n# daily Sharpe Ratio\ndaily_sr = lambda x: x.mean() / x.std()\n\n\nIm folgenden berechnen wir zunächst für jeden Handelstag die nicht-normierten Aktiengewichte (-1: Short; 0: Flat; 1: Long) durch Anwendung unserer Funktion weights_dc. Das resultierende DataFrame nennen wir port. Dann legen wir die Anzahl der Handelstage (hold) fest, für die wir das Portfolio halten wollen ohne die Gewichte umzuschichten.\nIm letzten Schritt reduzieren wir die Zeitfrequenz von port auf die Länge (hold) der gewählten Portfoliohalteperiode, setzen die Gewichte auf die Werte die zu Beginn der Halteperiode gelten (über .first()), und wählen .shift(1), da in t nur die Gewichte die zum Zeitpunkt t-1 bekannt sind implementiert werden können.\nDann berechnen wir die kumulativen Renditen jeder Aktie für die gewählte Portfoliohaltedauer. Hierzu verwenden wir die vorher definierte Lambda-Funktion compound.\nDanach multiplizieren wir für jede der einzelnen Halteperioden die kumulativen Aktienrenditen mit den Positionsindikatoren (-1, 0, 1), summieren die Produkte über alle Aktien auf, und teilen durch die Summe aller offen (Long und Short) Positionen um eine gleichgewichtete Portfoliorendite zu bekommen. Die resultierende Zeitreihe der kumulativen Portfoliorenditen nennen wir portf_rets. Sie hat dieselbe Zeitfrequenz wie freq, die Portfoliohaltedauer in Anzahl Handelstage. Wenden wir unsere Lambda-Funktion daily_srauf die Zeitreihe der Portfoliorenditen an und skalieren mit np.sqrt(252/hold) erhalten wir die annualisierte Sharpe-Ratio der Strategie.\nLassen Sie uns nun die obigen Schritte in der Strategie-Funktion strat_dc zusammenfassen. Wir benötigen ein DataFrame (df_returns) mit den Renditezeitreihen unseres Anlageuniversums und der gewünschten Risikofaktoren zur Berechnung der IVOL, und die Länge der Portfoliohaltedauer (hold).\n\n\nCode\n# strategy function\ndef strat_dc(df_returns, hold):\n    # calculation of security weights: (-1: short, 0: no, 1: long)\n    freq = '%dB' % hold # holding period \n    port = weights_dc(df_returns) # security weights at business day freq\n       \n    # daily returns without index; adjustment if more than one index!\n    df_returns0 = df_returns.drop('SP_Index', axis=1)\n\n    # adjustment of frequency for returns and weight time series\n    temp0 = port.iloc[:, 0]\n    temp0.name = 'temp'\n    daily_rets = pd.merge(df_returns0, temp0, how='inner', on='Date').drop('temp', axis=1)\n \n    # calculation of portfolio returns\n    port = port.shift(1).resample(freq).first() # time series with 'freq' as frequency\n    returns = daily_rets.resample(freq).apply(compound)\n    portf = np.multiply(port, returns) # security returns * position direction (-1, 0, 1)\n    # summing position returns divided by number of positions\n    portf_rets = portf.sum(axis = 1)/(portf != 0).sum(axis =1)\n    \n    return daily_sr(portf_rets) * np.sqrt(252/hold)\n\n\nFühren wir beispielhaft die Strategie als eine High-Risk Strategie (ascending=True) mit einer Halteperiode von 20 Handelstagen durch.\n\n\nCode\nstrat_dc(df_returns, 20)\n\n\n0.4300863729003128\n\n\n\n\n5.2.3 Implementierung 2: Gewichte nach Frazzini and Pedersen (2014)\nZur Implementierung der FP Gewichte erstellen wir das gewohnte (tägliche) DataFrame ranks mit den Perzentil-Rängen der Aktien gemäß ihrer IVOL über eine vergangene lookback (window) Periode. Daraus erstellen wir ein neues DataFrame demeaned mit Rangabweichungen indem wir jeweils den Zeilenmittelwert (ranks.mean(axis=1)) der Ränge vom Rang einer Aktie abziehen. Zusätzlich enthält das DataFrame abs_demeaned die absoluten Rangabweichungen. Wir bekommen die finalen FP Gewichte (DataFrame weights) indem wir die Zeilenwerte von demeaned durch die Hälfte der korrespondierenden Zeilensumme von abs_demeaned teilen.\nFassen wir diese Schritte nun in der Gewichtsfunktion weights_fp zusammen. Alle weiteren Schritte erfolgen analog zur obigen Vorgehensweise. Die Strategiefunktion nenne ich strat_fp.\n\n\nCode\ndef weights_fp(df_returns):\n    # weights according to Frazzini and Pedersen (2014), equation (16), p. 9\n    stdev = df_idiovola(df_returns)\n    # percentile ranks; \"False\"= long in stocks with low IVOL!\n    ranks = stdev.rank(ascending=True, axis=1, pct=True) \n    demeaned = ranks.sub(ranks.mean(axis=1), axis='index') # cross-sectional demeaned\n    abs_demeaned = abs(demeaned) \n    # demeaned percentile ranks normalized by 0.5 * cross-sectional sum of abs. demeaned weights\n    weights = demeaned.div(0.5 * abs_demeaned.sum(axis=1), axis='index')\n    return weights\n\n\n# strategie returns (main function)\ndef strat_fp(df_returns, hold):\n    # portfolio weights\n    freq = '%dB' % hold # holding period in number of business days\n    port = weights_fp(df_returns) # weights for each business day\n    \n    # daily returns without index; adjustment if more than one index!\n    df_returns0 = df_returns.drop('SP_Index', axis=1)\n\n    # adjustment of frequency for returns and weight time series\n    temp0 = port.iloc[:, 0]\n    temp0.name = 'temp'\n    daily_rets = pd.merge(df_returns0, temp0, how='inner', on='Date').drop('temp', axis=1)\n    \n    # strategy returns, shift(1) to adjust for implementation lag\n    port = port.shift(1).resample(freq).first() # time series with holding period freq\n    returns = daily_rets.resample(freq).apply(compound)\n    port_rets = (port * returns).sum(axis=1) # weighted sum of security returns \n    \n    return daily_sr(port_rets) * np.sqrt(252/hold)\n    \n\n\nFühren wir beispielhaft die Strategie als eine High-Risk Strategie (ascending=True) mit einer Halteperiode von 20 Handelstagen durch.\n\n\nCode\nstrat_fp(df_returns,20)\n\n\n0.1792046276890964",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risikobasierte Faktoren: Idiosyncratic Volatility - IVOL</span>"
    ]
  },
  {
    "objectID": "kapitel5.html",
    "href": "kapitel5.html",
    "title": "6  Risikobasierte Faktoren: Stock Beta",
    "section": "",
    "text": "6.1 Hintergrund und Motivation\nEine Basisimplikation des Capital Asset Pricing Modells (CAPM) ist, dass alle Investoren in das Portfolio mit der höchsten erwarteten Überschussrendite pro Risikoeinheit (Sharpe Ratio) investieren und ihre Position in diesem Portfolio entsprechend ihrer Risikopräferenz mit Fremdkapital entweder hebeln (“leveraging” - erhöhen) oder enthebeln (“de-leveraging” – reduzieren). Viele Anleger, wie z.B. Privatpersonen, Pensionsfonds und Investmentfonds, haben jedoch nur eine eingeschränkte (oder gar keine) Möglichkeiten, Leverage (Fremdkapital) einzusetzen, und müssen daher riskante (High-Beta) Wertpapiere übergewichten, anstatt gehebelte Positionen in Low-Beta Aktien einzugehen. Diese aus Leveragebeschränkungen resultierende Präferenz für High-Beta Aktien kann dazu führen, dass im Gleichgewicht risikoreiche Wertpapiere mit hohem Beta niedrigere risikobereinigte Renditen erfordern als Wertpapiere mit niedrigem Beta, für die eine Hebelwirkung (d.h., eine Position unter Einsatz von Fremdkapital) erforderlich ist. Im Ergebnis bedeutet dies: Low-Beta gleich High-Alpha, und High-Beta gleich Low-Alpha! (Alpha bezeichnet hier risikoadjustierte Renditen)\nTatsächlich ist die Wertpapiermarktlinie für US-Aktien im Vergleich zum CAPM zu flach (Black, Jensen und Scholes, 1972) und lässt sich durch das CAPM mit eingeschränkter Kreditaufnahme besser erklären als durch das Standard-CAPM (siehe schon Black, 1972).\nIn einem viel zitierten Aufsatz stellen Frazzini und Pedersen (FP) (“Betting against beta”, Journal of Financial Economics, 2014, p. 1-25) eine “Long minus Short” Faktorstrategie vor, mit dem Ziel die oben skizzierte Beta Anomalie auszunutzen. Sie konstruieren dabei einen “Betting Against Beta” (BAB) Faktor, der im Kern Low-Beta Aktien übergewichtet und High-Beta Aktien untergewichtet. Sie wenden die Faktorkonstruktion auf zahlreiche Assetklassen (US und 20 internationale Aktienmärkte, US-Staatsanleihen und Unternehmensanleihen, Future Märkte) an und können zeigen, dass die Faktorstrategie überall persistente, positive risikoadjustierte Renditen erwirtschaftet.\nIm folgenden implementieren wir die Basisversion der FP BAB Faktorstrategie.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Risikobasierte Faktoren: Stock Beta</span>"
    ]
  },
  {
    "objectID": "kapitel5.html#beginn-der-fallstudie",
    "href": "kapitel5.html#beginn-der-fallstudie",
    "title": "6  Risikobasierte Faktoren: Stock Beta",
    "section": "6.2 Beginn der Fallstudie",
    "text": "6.2 Beginn der Fallstudie\nWir beginnen mit dem Laden der notwendigen Pakete.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n\nWir setzen die BAB Faktorstrategie am Beispiel des S&P500 Universums um. Zunächst laden wir zwei Datensätze in Form von DataFrames, eines für die täglichen Aktienkurshistorien der S&P500 Mitglieder und eines für die Zeitreihe der Indexwerte des S&P500. Wir mergen beide Datensätze in dem DataFrame joined_df.\n\n\nCode\n# load of data: df with S&P500 constituents and df with S&P500 index values\n%cd \"C:\\Users\\Galina\\Documents\\Thomas\\Python Projekte\\examplefiles\"\nconstituents = pd.read_csv('s&p_500_15112019.csv', \n                   parse_dates=True, index_col=0)\nindex = pd.read_csv('s&p_500_012000_102019.csv',parse_dates=True, index_col=0) \nsp_index = index['Adj Close']\nsp_index.name = 'SP_Index'\n\n# adjustment for different frequency of constituents and index\njoined_df = pd.merge(constituents, sp_index, how='inner', on='Date')\n\n\nC:\\Users\\Galina\\Documents\\Thomas\\Python Projekte\\examplefiles\n\n\n\n\nCode\njoined_df.head(5)\n\n\n\n\n\n\n\n\n\nABT\nABBV\nABMD\nACN\nATVI\nADBE\nAMD\nAAP\nAES\nAMG\n...\nWYNN\nXEL\nXRX\nXLNX\nXYL\nYUM\nZBH\nZION\nZTS\nSP_Index\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-03\n36.866280\n54.056015\n112.360001\n110.738106\n35.941536\n103.480003\n11.43\n169.755737\n10.266887\n141.927734\n...\n81.712685\n37.324787\n25.328613\n55.914982\n47.835201\n60.320660\n101.052895\n40.778381\n52.556099\n2257.830078\n\n\n2017-01-04\n37.158947\n54.818222\n115.739998\n111.004356\n36.647812\n104.139999\n11.43\n171.148819\n10.178833\n145.451904\n...\n84.347366\n37.490185\n26.284410\n55.507954\n48.548149\n60.540157\n101.981964\n41.363899\n53.066067\n2270.750000\n\n\n2017-01-05\n37.479931\n55.233967\n114.809998\n109.340340\n37.216755\n105.910004\n11.24\n171.029388\n10.046756\n143.670349\n...\n85.431145\n37.490185\n26.174128\n54.835869\n48.104961\n60.740559\n102.637207\n40.693394\n52.889545\n2269.000000\n\n\n2017-01-06\n38.499535\n55.251297\n115.419998\n110.585968\n37.187328\n108.300003\n11.32\n168.790543\n10.407769\n143.349106\n...\n86.356071\n37.600452\n25.769753\n55.896046\n47.767757\n61.475353\n102.646980\n40.957817\n53.056259\n2276.979980\n\n\n2017-01-09\n38.461781\n55.615067\n117.110001\n109.349854\n36.981331\n108.570000\n11.49\n168.691055\n10.152417\n139.630264\n...\n86.655045\n37.030743\n25.622707\n55.905521\n47.565422\n61.647121\n104.642014\n40.513969\n52.909161\n2268.899902\n\n\n\n\n5 rows × 503 columns\n\n\n\nWir werden in drei Schritten die BAB Faktorstrategie von FP implementieren. Zunächst schreiben wir eine Funktion (calc_beta), die den renditeprognostizierenden Faktor (das Aktienbeta zum Marktindex) für jede Aktie täglich auf Basis rollierender Zeitfenster berechnet. Danach verwenden wir diesen Faktor in der Funktion bab_weights um zwei Faktorportfolios zu bilden. Das Long (Low-Beta) Portfolio enthält Aktien mit niedrigem Beta, und das Short (High-Beta) Portfolio beinhaltet die Aktien, die stärker mit dem Markt variieren. In der dritten Funktion bab_strat führen wir alles zusammen, konstruieren den BAB “Long minus Short” Faktor, wobei die beiden Portfolios jeweils mit der Inversen ihres Betas skaliert werden, und berechnen für eine gegebene Halteperiode die annualisierte Sharpe-Ratio der Faktorstrategie.\n\n6.2.1 Ex Ante Betas als Faktor\nDas geschätze \\(\\beta\\) für Aktie \\(i\\) ist definiert durch (siehe FP, 2014, Gleichung 14):\n\\[\\beta^{TS}_{i}=\\rho\\frac{\\sigma_{i}}{\\sigma_{m}}\\]\nHierbei sind \\(\\sigma_{i}\\) und \\(\\sigma_{m}\\) die historisch geschätzen Renditevolatilitäten der Aktie \\(i\\) und des Marktes und \\(\\rho\\) deren geschätzte Korrelation. FP schätzen Volatilitäten und Korrelationen separat. Konkret verwenden sie 1-Tages Log-Renditen für Volatilitäten und überlappende 3-Tages Log-Renditen für Korrelationen. Die jeweiligen gleitenden Zeitfenster für die täglichen rollierenden Schätzungen betragen 1 Jahr bei Volatilitäten und 5 Jahre für Korrelationen.\nUm den Einfluss von Ausreißern zu mindern werden die obigen (täglichen) Zeitreihen-Betas \\(\\beta^{TS}_{i}\\) in Richtung des Querschnittsmittelwertes \\(\\beta^{XS}\\) über folgende Gleichung geschrumpft (sogenannte “Shrinkage”-Schätzung; siehe FP, 2014, Gleichung 15):\n\\[\\beta_{i}=w_{i}\\beta^{TS}_{i}+(1-w_{i})\\beta^{XS}\\]\nZur Vereinfachung setzen FP \\(w=0.6\\) und \\(\\beta^{XS}=1\\) für alle Zeitpunkte und Wertpapiere.\nWir implementieren die Berechnung der FP Betas in der Funktion calc_beta. Diese benötigt als Eingabe ein DataFrame mit täglichen Historien an Aktienkursen und Marktindexwerten. Über apply(lambda...) erstellen wir jeweils DataFrames mit täglichen Zeitreihen von 1-Tages und überlappenden (kumulativen) 3-Tages Log-Renditen. Diese DataFrames werden über die rolling-Funktionalität transformiert in DataFrames mit Standardabweichungen (stdev), dem Verhältnis der Aktienvolatilität zur Marktvolatilität (stdev_ratio), und Renditekorrelationen (corr) mit dem Marktindex (Spalte ‘SP_Index’). Die Zeitreihenbetas \\(\\beta^{TS}_{i}\\) ergeben sich aus der zellenweisen Multiplikation von corr und stdev_ratio. Die geschrumpften Betas \\(\\beta_{i}\\) sind im finalen DataFrame beta_shrink enthalten.\n\n\nCode\n# calculation of Frazzini and Pedersen (2014) beta (equations (14) and (15))\ndef calc_beta(joined_df):\n    joined_df = joined_df.asfreq('B').fillna(method='pad')\n    # calculation of log-returns (1 day and 3 days-overlapping)\n    log1_rets = joined_df.apply(lambda x: np.log(x/x.shift(1)))\n    log3_rets = joined_df.apply(lambda x: np.log(x/x.shift(3)))\n\n    # individual calculation of volas, correlation and finally betas\n    stdev = log1_rets.rolling(250, min_periods=120).std()\n    stdev_ratio = stdev.div(stdev['SP_Index'], axis=0)\n    corr = log3_rets.rolling(250, min_periods=120).corr(log3_rets['SP_Index'])\n    beta = np.multiply(corr, stdev_ratio)\n    beta_shrink = 0.6 * beta + 0.4\n    return beta_shrink\n\n\n\n\n6.2.2 Konstruktion der Long-/Short-Portfolios\nUm ihren BAB-Faktor zu konstruieren, ordnen FP alle Wertpapiere im Datensatz zu jedem Zeitpunkt in aufsteigender Reihenfolge auf Grundlage der geschätzten Betas an (Perzentil-Ranking). Die geordneten Wertpapiere werden einem von zwei Portfolios zugewiesen: dem Low-Beta (\\(L\\)) und dem High-Beta (\\(H\\)) Portfolio. Der FP BAB Faktor basiert darauf, Long in das Low-Beta und Short in das High-Beta Portfolio zu gehen. Das Low- (High-) Beta Portfolio setzt sich aus allen Aktien zusammen, deren Beta unter (über) dem Median-Beta aller Wertpapiere im Datensatz liegt. In jedem Portfolio werden Wertpapiere anhand des Perzentil-Rangs ihres geschätzten Betas gewichtet. D.h., im Low-Beta Portfolio haben Wertpapiere mit niedrigerem Beta größere Gewichte, und im High-Beta Portfolio steigt entsprechend das Gewicht für Wertpapiere mit höherem Beta. Formal ergeben sich die Gewichtsvektoren der Aktien in beiden Portfolios wie folgt (siehe FP, 2014, Gleichung 16):\n\\[w_{H}=k(z-\\mu_{z})^+\\] \\[w_{L}=k(z-\\mu_{z})^-\\]\nHierbei bezeichnet \\(z\\) einen \\(n x 1\\) Vektor der Beta-Perzentil-Ränge \\(z_{i}=rank(\\beta_{it})\\) zum Zeitpunkt der Portfoliokonstruktion, \\(\\mu_{z}=1_{n}z/n\\) den durchschnittlichen Rang, \\(n\\) die Anzahl der Aktien und \\(1_{n}\\) den Einheitsvektor der Dimension \\(n x 1\\). \\(x^+\\) und \\(x^-\\) kennzeichnen die positiven bzw. negativen Elemente eines Vektors \\(x\\). Um sicher zu stellen, dass sich die Gewichte in beiden Portfolios zu 1 summieren, d.h., \\(1_{n}w_{H}=1\\) bzw. \\(1_{n}w_{L}=1\\) gilt, wird die Normalisierungskonstante \\(k\\) definiert als \\(k=2/1_{n}|z-\\mu_{z}|\\).\nIn Worten ergibt sich das Gewicht jeder Aktie aus der Rangabweichung der Aktie vom mittleren Rang skaliert mit der (d.h. geteilt durch die) Hälfte der Summe der absoluten Rangabweichungen über alle Aktien.\nZur Implementierung der FP Portfolios verwenden wir das tägliche DataFrame ranks mit den Perzentil-Rängen der Aktien gemäß ihres geschätzten Betas. Daraus erstellen wir ein neues DataFrame demeaned mit Rangabweichungen indem wir jeweils den Zeilenmittelwert (ranks.mean(axis=1)) der Ränge vom Rang einer Aktie abziehen.\nIm nächsten Schritt generieren wir zwei transformierte Versionen von demeaned, die DataFrames long und short, mit Indikatoren (0/1-Variablen), die jeweils anzeigen, ob eine Aktie an einen entsprechenden Tag in das Long (High-Beta) oder das Short (Low-Beta) Portfolio gehört.\nDann erstellen wir das DataFrame abs_demeaned mit den absoluten Rangabweichungen. Wir bekommen die normalisierten Gewichte (DataFrame weights) indem wir die Zeilenwerte von abs_demeaned durch die Hälfte der korrespondierenden Zeilensumme von abs_demeaned teilen.\nWir erhalten die beiden DataFrames long_weights und short_weights mit den täglichen Zeitreihen der Portfoliogewichtsvektoren indem wir das DataFrame weights jeweils mit den DataFrames der Portfolio-Positionsindikatoren multiplizieren.\nIm letzten Schritt berechnen wir die Zeitreihe der beiden Portfolio-Betas durch zellenweise Multiplikation des DataFrames der Aktienbeta-Zeitreihen (beta) mit den DataFrames der Portfoliogewichte und Bilden der Zeilensummen.\nFassen wir diese Schritte nun in der Gewichtsfunktion bab_weights zusammen. Diese Funktion benötigt als Eingabe ein DataFrame mit Aktienkurshistorien und Indexwerten. Als Ausgabe der Funktion erhalten wir die beiden DataFrames mit den Portfoliogewichten zu jedem Zeitpunkt, und zwei Zeitreihen mit den gewichteten Betas der Portfolios.\n\n\nCode\n# calculation of weights, equation (16) in Frazzini and Pedersen (2014)\ndef bab_weights(joined_df):\n    beta_shrink = calc_beta(joined_df)\n    beta_shrink = beta_shrink.dropna(thresh=10) # min 10 obs each row with non-missing betas\n    beta = beta_shrink.drop('SP_Index', axis=1)\n    ranks = beta.rank(ascending=True, axis=1, pct=True) # percentile ranks\n    demeaned = ranks.sub(ranks.mean(axis=1), axis='index') # cross-sectional demeaned\n\n    # indicator matrix: 1 for long position (demeaned beta negativ)\n    long = demeaned.copy()\n    for col in long.columns:\n        long[col] = np.where(long[col]&lt;0, 1, 0)\n    \n    #indicator matrix: 1 for short position (demeaned beta positiv)\n    short = demeaned.copy()\n    for col in short.columns:\n        short[col] = np.where(short[col]&gt;0, 1, 0)\n    \n    # calculation of normalized weights    \n    abs_demeaned = abs(demeaned) \n    # demeaned percentile ranks normalized by 0.5 * cross-sectional sum of abs. demeaned weights\n    weights = abs_demeaned.div(0.5 * abs_demeaned.sum(axis=1), axis='index')\n\n    # separate matrix for long and short weights\n    long_weights = np.multiply(long, weights)\n    short_weights = np.multiply(short, weights)\n\n    # calculation of long and short portfolio betas\n    long_beta = (beta * long_weights).sum(axis=1)\n    short_beta = (beta * short_weights).sum(axis=1)\n    return long_weights, short_weights, long_beta, short_beta\n\n\nBevor wir mit dem dritten Schritt, der Konstruktion des BAB Faktors, weitermachen, definieren wir unsere bekannten Lambda Hilfs-Funktionen:\n1. \"compound\" berechnet aus dem Eingabe-Array x kumulative Mehr-Tagesrenditen;\n2. \"daily_sr\" berechnet aus einem Array von Tagesrenditen die tägliche Sharpe-Ratio;\n\n\nCode\n# cumulative returns\ncompound = lambda x: (1 + x).prod() - 1\n\n# daily sharpe ratio\ndaily_sr = lambda x: x.mean() / x.std()\n\n\n\n\n6.2.3 Konstruktion des BAB Faktors\nKonstruktionsbedingt ist das Portfoliobeta im Long (Low-Beta) Portfolio immer kleiner als das im Short (High-Beta) Portfolio. Wie lässt sich nun eine Long-Short BAB Faktorstrategie implementieren, die marktneutral (Zero-Beta Portfolio) ist? Hierzu müssen die Positionen in den beiden Portfolios gehebelt werden. Konkret benötigen wir eine “leveraged” Position im Low-Beta Portfolio und eine “deleveraged” Position im High-Beta Portfolio. Das Ziel ist es hierbei, dass die Positionen in den beiden Portfolios jeweils ein Beta von Eins haben. Hat das \\(L\\) (\\(H\\)) Portfolio beispielsweise ein Beta von 0,75 (1,4), müssen 1,33 (0,7) Geldeinheiten in das Portfolio investiert werden. Die Finanzierung der Positionen erfolgt dabei über entsprechend entgegengesetzte Positionen im risikolosen Zins um die Strategie selbstfinanzierend zu halten. Wichtig: Eine Geldeinheit Long und eine Geldeinheit Short führen nicht zu einem Zero-Beta Faktor! Formal ergibt sich die BAB Faktorstrategierendite folglich als (siehe FP, 2014, Gleichung 17):\n\\[r^{BAB}_{t+1}=\\frac{1}{\\beta^L_{t}}(r^L_{t+1}-r_{f})-\\frac{1}{\\beta^H_{t}}(r^H_{t+1}-r_{f}),\\]\nmit \\(r^L_{t+1}=r^´_{t+1}w_{L}\\), \\(r^H_{t+1}=r^´_{t+1}w_{H}\\), \\(\\beta^L_{t}=\\beta^´_{t}w_{L}\\), und \\(\\beta^H_{t}=\\beta^´_{t}w_{H}\\). Im folgenden unterstellen wir zur Vereinfachung einen risikolosen Zins \\(r_{f}\\) von Null.\nWir implementieren nun die Konstruktion der marktneutralen BAB Faktorstrategie in der Funktion bab_strat. Wie immer halten wir das Faktorportfolio konstant für eine Anzahl von Handelstagen, die durch das Argument hold angegeben wird.\nWichtig: Wir reduzieren die Zeitfrequenz der DataFrames mit den täglichen (Long/Short) Portfoliogewichtsvektoren und der Zeitreihen der Portfoliobetas auf die Länge (hold) der gewählten Portfoliohalteperiode, setzen die Gewichte/Betas auf die Werte die zu Beginn der Halteperiode gelten (über .first()), und wählen .shift(1), da in t nur die Gewichte die zum Zeitpunkt t-1 bekannt sind implementiert werden können.\nWir berechnen separate Zeitreihen der kumulierten Halteperioderenditen für das Long- und das Short-Portfolio (long_rets bzw. short_rets). Die Faktorrendite ergibt sich als Differenz (Long minus Short) der Portfoliorenditen, jeweils skaliert mit der Inversen des Portfoliobetas.\nZusätzlich zur annualisierten Sharpe-Ratio der Strategie gibt uns die Funktion auch die Sharpe-Ratio der Benchmark, d.h., der S&P500 Indexrenditen, zurück.\n\n\nCode\n# calculation of bab factor and strategy returns:\n# equation (17) in Frazzini and Pedersen (2014)\ndef bab_strat(joined_df, hold):\n    freq = '%dB' % hold # holding period in number of business days\n    # weights and portfolio betas for each business day\n    long_weights, short_weights, long_beta, short_beta = bab_weights(joined_df)\n\n    # daily returns\n    daily_rets = joined_df.pct_change()\n    long_beta.name ='long_beta'  \n    # adjustment of frequency for returns and beta/weights time series\n    daily_rets = pd.merge(daily_rets, long_beta, how='inner', on='Date').drop('long_beta', axis=1)\n    \n    # calculation of benchmark sharpe ratio\n    bench_rets = daily_rets['SP_Index']\n    bench_sharpe = daily_sr(bench_rets) * np.sqrt(252)\n    daily_rets = daily_rets.drop('SP_Index', axis=1)\n    \n    # taking weights and betas from the beginning of the holding period\n    long_weights = long_weights.shift(1).resample(freq).first() # time series with holding period freq\n    long_beta = long_beta.shift(1).resample(freq).first() # time series with holding period freq\n    short_weights = short_weights.shift(1).resample(freq).first() # time series with holding period freq\n    short_beta = short_beta.shift(1).resample(freq).first() # time series with holding period freq\n\n    # calculating holding period returns for long and short portfolio\n    returns = daily_rets.resample(freq).apply(compound)\n    long_rets = (long_weights * returns).sum(axis=1) # weighted sum of security returns \n    short_rets = (short_weights * returns).sum(axis=1) # weighted sum of security returns \n\n    # finally, calculating strategy returns (equation (17))\n    bab_rets = (1/long_beta * long_rets) - (1/short_beta * short_rets)\n    bab_sharpe = daily_sr(bab_rets) * np.sqrt(252/hold)\n    \n    return bab_sharpe, bench_sharpe\n\n\nBerechnen wir nun die Sharpe-Ratios der BAB Strategie mit einer Rebalancingfrequenz von 20 Tagen und der passiven Benchmark (S&P500).\n\n\nCode\nbab_strat(joined_df, 20)\n\n\n(0.9439961839745289, 0.7395384250144774)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Risikobasierte Faktoren: Stock Beta</span>"
    ]
  }
]